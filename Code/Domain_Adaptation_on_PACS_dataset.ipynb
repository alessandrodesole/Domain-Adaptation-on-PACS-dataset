{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Domain Adaptation on PACS dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9QcGnGPdX2C"
      },
      "source": [
        "\n",
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9O3aM3Tb28q"
      },
      "source": [
        "#!pip3 install 'torch==1.3.1'\n",
        "#!pip3 install 'torchvision==0.5.0'\n",
        "#!pip3 install 'Pillow-SIMD'\n",
        "#!pip3 install 'tqdm'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg91p_DGnDT2"
      },
      "source": [
        "**Import models and utils from GitHub**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd6cYhz5nVB7",
        "outputId": "0e82979b-d31c-4da3-cb0d-07f3e6d52914",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir('./models'):\n",
        "  !git clone https://github.com/alessandrodesole/Domain-adaptation-on-PACS-dataset.git\n",
        "  !cp -r \"/content/Domain-adaptation-on-PACS-dataset/Code/Models\" \"/content/\"\n",
        "  !cp -r \"/content/Domain-adaptation-on-PACS-dataset/Code/Utils\" \"/content/\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Domain-adaptation-on-PACS-dataset'...\n",
            "remote: Enumerating objects: 38, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 38 (delta 8), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (38/38), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIlP4yv0wbD_"
      },
      "source": [
        "**Import dataset from github**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTg1PoJyiR6l",
        "outputId": "fa2cfcb5-d450-4db7-bf52-ab6ba81bf048",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Clone github repository with data\n",
        "import os\n",
        "\n",
        "if not os.path.isdir('./Homework3-PACS'):\n",
        "  !git clone https://github.com/MachineLearning2020/Homework3-PACS\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Homework3-PACS'...\n",
            "remote: Enumerating objects: 10032, done.\u001b[K\n",
            "remote: Total 10032 (delta 0), reused 0 (delta 0), pack-reused 10032\u001b[K\n",
            "Receiving objects: 100% (10032/10032), 174.13 MiB | 40.12 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n",
            "Checking out files: 100% (9993/9993), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo942LMOdlh4"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DokFOdD1dJEl"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader, ConcatDataset\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import time\n",
        "import copy\n",
        "\n",
        "from models.AlexNetDANN import alexnet\n",
        "from Utils.utils import *"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIDLJuIXK_vh"
      },
      "source": [
        "**Set Arguments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5PkYfqfK_SA"
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "\n",
        "NUM_CLASSES = 7      # 7 classes for each domain\n",
        "\n",
        "BATCH_SIZE = 64     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 1e-4            # The initial Learning Rate\n",
        "FIX_LR = True\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 30      # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = 20       # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "ALPHA = 0.5\n",
        "FIX_ALPHA = True\n",
        "\n",
        "LOG_FREQUENCY = 10\n",
        "\n",
        "# OPTIONS -----------------\n",
        "\n",
        "KL = False\n",
        "CHECK_CLASSES = True\n",
        "COMPUTE_MEANS = False\n",
        "COMPUTE_DISTRIBUTION = False\n",
        "PLOT_DISTRIBUTION = False\n",
        "IMAGE_SHOW = False\n",
        "TRAIN_1 = False\n",
        "TRAIN_2 = False\n",
        "TRAIN_DANN = False\n",
        "VALIDATION = False\n",
        "VALIDATION_DANN = True\n",
        "TEST_WITHOUT_DANN = True\n",
        "TEST_WITH_DANN = True\n",
        "\n",
        "CARTOON = False\n",
        "SKETCH = True"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gwii0TBHvzh"
      },
      "source": [
        "**Define Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUDdw4j2H0Mc"
      },
      "source": [
        "# Define transforms for training phase\n",
        "train_transform = transforms.Compose([#transforms.Resize(227),      # Resizes short size of the PIL image to 227\n",
        "                                      transforms.CenterCrop(224),  # Crops a central square patch of the image\n",
        "                                                                   # 224 because torchvision's AlexNet needs a 224x224 input!\n",
        "                                                                   # Remember this when applying different transformations, otherwise you get an error\n",
        "                                      #transforms.RandomCrop( 64 , padding =2) ,\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalizes tensor with mean and standard deviation\n",
        "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                                      #transforms.Normalize((0.5085, 0.4832, 0.4396), 0.2776, 0.2693, 0.2867)\n",
        "])\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([#transforms.Resize(227),\n",
        "                                    transforms.CenterCrop(224),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                                    #transforms.Normalize((0.5085, 0.4832, 0.4396), 0.2776, 0.2693, 0.2867)                                    \n",
        "])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qYIHPzYLY7i"
      },
      "source": [
        "**Prepare Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfVq_uDHLbsR",
        "outputId": "bf43cf2a-5e8e-4b00-a8ab-d25f70abe271",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# All domains analysis\n",
        "\n",
        "DOMAIN_DATA_DIR = 'Homework3-PACS/PACS'\n",
        "\n",
        "# Prepare Pytorch train/test Datasets\n",
        "domains_dataset = torchvision.datasets.ImageFolder(DOMAIN_DATA_DIR, transform=train_transform)\n",
        "\n",
        "domains = domains_dataset.classes\n",
        "domains_to_idx = domains_dataset.class_to_idx\n",
        "\n",
        "# Check dataset size\n",
        "print('Dataset: {}'.format(len(domains_dataset)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 9991\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpSY3qtq4wmS",
        "outputId": "250c9eb1-9389-43a1-df50-c7bf6b9f660a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# All classes analysis\n",
        "\n",
        "DATA_DIRs = []\n",
        "DATA_DIRs.append('Homework3-PACS/PACS/art_painting')\n",
        "DATA_DIRs.append('Homework3-PACS/PACS/cartoon')\n",
        "DATA_DIRs.append('Homework3-PACS/PACS/photo')\n",
        "DATA_DIRs.append('Homework3-PACS/PACS/sketch')\n",
        "\n",
        "class_datasets = []\n",
        "\n",
        "for d in DATA_DIRs:\n",
        "  class_datasets.append(torchvision.datasets.ImageFolder(d, transform=train_transform))\n",
        "\n",
        "classes = class_datasets[0].classes\n",
        "class_to_idx = class_datasets[0].class_to_idx\n",
        "\n",
        "class_dataset = ConcatDataset(class_datasets)\n",
        "\n",
        "# Check dataset sizes\n",
        "print('Dataset: {}'.format(len(class_dataset)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 9991\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3UlOQPFX5Ic",
        "outputId": "911a7ecf-4bfe-49ae-8592-dd77e63aad10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "TRAIN_DATA_DIR = 'Homework3-PACS/PACS/photo'\n",
        "TEST_DATA_DIR = 'Homework3-PACS/PACS/art_painting'\n",
        "\n",
        "if VALIDATION or VALIDATION_DANN:\n",
        "  if CARTOON:\n",
        "    VAL_DATA_DIR = 'Homework3-PACS/PACS/cartoon'\n",
        "\n",
        "  if SKETCH:\n",
        "    VAL_DATA_DIR = 'Homework3-PACS/PACS/sketch'\n",
        "\n",
        "# Prepare Pytorch train/test Datasets\n",
        "train_dataset = torchvision.datasets.ImageFolder(TRAIN_DATA_DIR, transform=train_transform)\n",
        "test_dataset = torchvision.datasets.ImageFolder(TEST_DATA_DIR, transform=eval_transform)\n",
        "\n",
        "if VALIDATION or VALIDATION_DANN:\n",
        "  val_dataset = torchvision.datasets.ImageFolder(VAL_DATA_DIR, transform=train_transform)\n",
        "\n",
        "classes = train_dataset.classes\n",
        "class_to_idx = train_dataset.class_to_idx\n",
        "\n",
        "# Check dataset sizes\n",
        "print('Train Dataset: {}'.format(len(train_dataset)))\n",
        "print('Test Dataset: {}'.format(len(test_dataset)))\n",
        "\n",
        "if VALIDATION or VALIDATION_DANN:\n",
        "  print('Validation Dataset: {}'.format(len(val_dataset)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Dataset: 1670\n",
            "Test Dataset: 2048\n",
            "Validation Dataset: 3929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUdnm5FRnxr6",
        "outputId": "86b78383-1ec3-4d36-f569-2635fbdc6ee4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check if classes are distributed equally between training and test set\n",
        "\n",
        "if CHECK_CLASSES:\n",
        "  if VALIDATION or VALIDATION_DANN:\n",
        "    check_classes(train_dataset, test_dataset, val_dataset)\n",
        "  else:\n",
        "    check_classes(train_dataset, test_dataset)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    189\n",
            "1    202\n",
            "2    182\n",
            "3    186\n",
            "4    199\n",
            "5    280\n",
            "6    432\n",
            "dtype: int64\n",
            "0    379\n",
            "1    255\n",
            "2    285\n",
            "3    184\n",
            "4    201\n",
            "5    295\n",
            "6    449\n",
            "dtype: int64\n",
            "0    772\n",
            "2    753\n",
            "4    816\n",
            "6    160\n",
            "1    740\n",
            "3    608\n",
            "5     80\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p-g1rVhfCDi"
      },
      "source": [
        " **Kullback-Leibler divergence**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKefOqih-gF7"
      },
      "source": [
        "if KL:\n",
        "  compute_kl(train_dataset, test_dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POWiWairs54S"
      },
      "source": [
        "**Compute means**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6Wtpe1tpRKa"
      },
      "source": [
        "if COMPUTE_MEANS:\n",
        "\n",
        "  compute_means(train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSP5nd7o96x7"
      },
      "source": [
        "**Images distribution among domains/classes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT4IQZ8--Bl0"
      },
      "source": [
        "if COMPUTE_DISTRIBUTION:\n",
        "\n",
        "  compute_plot_distribution(classes, domains, train_dataset, test_dataset, val_dataset=None, class_dataset, domains_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58duJUn1mTo5"
      },
      "source": [
        "**Visualize few images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rFC5JHcmXAz"
      },
      "source": [
        "# I want to print one image per class from training set\n",
        "\n",
        "if IMAGE_SHOW:\n",
        "\n",
        "  visited_class = set()\n",
        "  # Get a batch of training data\n",
        "  for inputs, d in domains_dataset:\n",
        "    if d not in visited_class:\n",
        "      # Make a grid from batch\n",
        "      out = torchvision.utils.make_grid(inputs)\n",
        "      imshow(out, title=domains[d])\n",
        "      visited_class.add(d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYEDQ7Z21ldN"
      },
      "source": [
        "**Prepare Dataloaders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VriRw8SI1nle"
      },
      "source": [
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n",
        "\n",
        "if VALIDATION or VALIDATION_DANN:\n",
        "  val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbZ1t5Qs2z4j"
      },
      "source": [
        "**Prepare Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exHUjtXa22DN"
      },
      "source": [
        "net = alexnet(pretrained=True, num_classes=NUM_CLASSES) # Loading AlexNet model\n",
        "\n",
        "# Copy pretrained weights from the class classifier to the domain_classifier\n",
        "net.domain_classifier[1].weight.data = net.class_classifier[1].weight.data\n",
        "net.domain_classifier[1].bias.data = net.class_classifier[1].bias.data\n",
        "net.domain_classifier[4].weight.data = net.class_classifier[4].weight.data\n",
        "net.domain_classifier[4].bias.data = net.class_classifier[4].bias.data\n",
        "net.class_classifier[6] = nn.Linear(4096, NUM_CLASSES) # nn.Linear in pytorch is a fully connected layer\n",
        "                                                 # The convolutional layer is nn.Conv2d\n",
        "net.domain_classifier[6] = nn.Linear(4096, 2)\n",
        "# We just changed the last layer of AlexNet with a new fully connected layer with 101 outputs\n",
        "# It is mandatory to study torchvision.models.alexnet source code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEyL3H_R4qCf"
      },
      "source": [
        "**Prepare Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sjq00G94tSc"
      },
      "source": [
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n",
        "\n",
        "# Choose parameters to optimize\n",
        "# To access a different set of parameters, you have to access submodules of AlexNet\n",
        "# (nn.Module objects, like AlexNet, implement the Composite Pattern)\n",
        "# e.g.: parameters of the fully connected layers: net.classifier.parameters()\n",
        "# e.g.: parameters of the convolutional layers: look at alexnet's source code ;) \n",
        "parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters of AlexNet\n",
        "\n",
        "# Define optimizer\n",
        "# An optimizer updates the weights based on loss\n",
        "# We use SGD with momentum\n",
        "if FIX_LR:\n",
        "  optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "else:\n",
        "  optimizer = optim.SGD(parameters_to_optimize, lr=1., momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "#optimizer = optim.Adam(parameters_to_optimize, lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "#for p in net.parameters():\n",
        "#  p.requires_grad = True\n",
        "\n",
        "# Define scheduler\n",
        "# A scheduler dynamically changes learning rate\n",
        "# The most common schedule is the step(-down), which multiplies learning rate by gamma every STEP_SIZE epochs\n",
        "\n",
        "if FIX_LR:\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "else:\n",
        "  lambda1 = lambda epoch: 0.01 / ((1 + 10 * (epoch / NUM_EPOCHS)) ** 0.75)\n",
        "  scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxYUli9d9uYQ"
      },
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcoQ5fD49yT_"
      },
      "source": [
        "if TRAIN_1:\n",
        "\n",
        "  # By default, everything is loaded to cpu\n",
        "  net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "\n",
        "  cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "  current_step = 0\n",
        "  train_losses = []\n",
        "\n",
        "  # Start iterating over the epochs\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
        "\n",
        "    running_loss = 0.0\n",
        "    # Iterate over the dataset\n",
        "    for images, labels in train_dataloader:\n",
        "      # Bring data over the device of choice\n",
        "\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "\n",
        "      net.train() # Sets module in training mode\n",
        "\n",
        "      # PyTorch, by default, accumulates gradients after each backward pass\n",
        "      # We need to manually set the gradients to zero before starting a new iteration\n",
        "      optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "      # Forward pass to the network\n",
        "      outputs = net(images)\n",
        "\n",
        "      # Compute loss based on output and ground truth\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      # Log loss\n",
        "      if current_step % LOG_FREQUENCY == 0:\n",
        "        print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "      # Compute gradients for each layer and update weights\n",
        "      loss.backward()  # backward pass: computes gradients\n",
        "      optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "      current_step += 1\n",
        "      running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / float(len(train_dataloader)*BATCH_SIZE)\n",
        "    print(\"Train epoch loss: {}\".format(epoch_loss))\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    # Step the scheduler\n",
        "    scheduler.step() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiovHQhiq9iY"
      },
      "source": [
        "if TRAIN_2:\n",
        "\n",
        "  since = time.time()\n",
        "\n",
        "  print(\"Current Hyperparameters:\")\n",
        "  print(f\"Epochs = {NUM_EPOCHS}\")\n",
        "  print(f\"  Scheduler:\")\n",
        "  print(f\"  - Step Size = {STEP_SIZE}\")\n",
        "  print(f\"  - Gamma = {GAMMA}\")\n",
        "  print(f\"Optimizer: {optimizer}\")\n",
        "  print(f\"Device: {DEVICE}\")\n",
        "  print()\n",
        "\n",
        "  train_loss = []\n",
        "  train_acc = []\n",
        "\n",
        "  # By default, everything is loaded to cpu\n",
        "  net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "\n",
        "  cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "  current_step = 0\n",
        "  # Start iterating over the epochs\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
        "\n",
        "    net.train() # Sets module in training mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    # Iterate over the dataset\n",
        "    for images, labels in train_dataloader:\n",
        "      # Bring data over the device of choice\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "\n",
        "      # PyTorch, by default, accumulates gradients after each backward pass\n",
        "      # We need to manually set the gradients to zero before starting a new iteration\n",
        "      optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "      with torch.set_grad_enabled(True):\n",
        "\n",
        "        # Forward pass to the network\n",
        "        outputs = net(images)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Compute loss based on output and ground truth\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Log loss\n",
        "        if current_step % LOG_FREQUENCY == 0:\n",
        "          print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "        # Compute gradients for each layer and update weights\n",
        "        loss.backward()  # backward pass: computes gradients\n",
        "        optimizer.step() # update weights based on accumulated gradients\n",
        "        \n",
        "      current_step += 1\n",
        "\n",
        "      # statistics\n",
        "      running_loss += loss.item() * images.size(0)\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item() \n",
        "    \n",
        "    epoch_loss = running_loss / float(len(train_dataloader)*BATCH_SIZE)\n",
        "    epoch_acc = running_corrects / float(len(train_dataloader)*BATCH_SIZE)\n",
        "\n",
        "    train_loss.append(epoch_loss) # loss computed as the average on mini-batches\n",
        "    #train_loss.append(loss) # loss computed only on the last batch\n",
        "    train_acc.append(epoch_acc)\n",
        "\n",
        "    print('Train -> Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "\n",
        "    # Step the scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    print()\n",
        "\n",
        "  time_elapsed = time.time() - since\n",
        "  print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeOFDekZz18s"
      },
      "source": [
        "**DANN Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ot-W8m7K5sW"
      },
      "source": [
        "if TRAIN_DANN:\n",
        "\n",
        "  since = time.time()\n",
        "\n",
        "  print(\"Current Hyperparameters:\")\n",
        "  print(f\"Epochs = {NUM_EPOCHS}\")\n",
        "  print(f\"  Scheduler:\")\n",
        "  print(f\"  - Step Size = {STEP_SIZE}\")\n",
        "  print(f\"  - Gamma = {GAMMA}\")\n",
        "  print(f\"Optimizer: {optimizer}\")\n",
        "  print(f\"Device: {DEVICE}\")\n",
        "  print(f\"Alpha: {ALPHA}\")\n",
        "  print()\n",
        "\n",
        "  train_class_losses = []\n",
        "  train_domain_losses = []\n",
        "  test_domain_losses = []\n",
        "  train_acc = []\n",
        "\n",
        "  # By default, everything is loaded to cpu\n",
        "  net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "\n",
        "  cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "  current_step = 0\n",
        "  # Start iterating over the epochs\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
        "\n",
        "    len_dataloader = min(len(train_dataloader), len(test_dataloader))\n",
        "    data_train_iter = iter(train_dataloader)\n",
        "    data_test_iter = iter(test_dataloader)\n",
        "\n",
        "    running_train_class_losses = 0\n",
        "    running_train_domain_losses = 0\n",
        "    running_test_domain_losses = 0\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    while i < len_dataloader:\n",
        "\n",
        "      if FIX_ALPHA == False:\n",
        "        p = float(i + epoch * len_dataloader) / NUM_EPOCHS / len_dataloader\n",
        "        ALPHA = 2. / (1. + np.exp(-10 * p)) - 1\n",
        "        print(\"Alpha = {}\\n\".format(ALPHA))\n",
        "\n",
        "      # training model using source data\n",
        "      data_train = data_train_iter.next()\n",
        "      train_img, train_label = data_train\n",
        "\n",
        "      #net.zero_grad()\n",
        "      optimizer.zero_grad()\n",
        "      batch_size = len(train_label)\n",
        "\n",
        "      input_img = torch.FloatTensor(batch_size, 3, 224, 224)\n",
        "      class_label = torch.LongTensor(batch_size)\n",
        "      domain_label = torch.zeros(batch_size)\n",
        "      domain_label = domain_label.long()\n",
        "\n",
        "      train_img = train_img.to(DEVICE)\n",
        "      train_label = train_label.to(DEVICE)\n",
        "      input_img = input_img.to(DEVICE)\n",
        "      class_label = class_label.to(DEVICE)\n",
        "      domain_label = domain_label.to(DEVICE)\n",
        "\n",
        "      input_img.resize_as_(train_img).copy_(train_img)\n",
        "      class_label.resize_as_(train_label).copy_(train_label)\n",
        "\n",
        "      train_class_output = net(input_img)\n",
        "      train_class_loss = criterion(train_class_output, class_label)\n",
        "\n",
        "      running_train_class_losses += train_class_loss.item() * train_img.size(0)\n",
        "\n",
        "      # Compute gradients for each layer and update weights\n",
        "      train_class_loss.backward()  # backward pass: computes gradients\n",
        "\n",
        "      train_domain_output = net(input_img, alpha=ALPHA)\n",
        "      train_domain_loss = criterion(train_domain_output, domain_label)\n",
        "\n",
        "      running_train_domain_losses += train_domain_loss.item() * train_img.size(0)\n",
        "\n",
        "      # Compute gradients for each layer and update weights\n",
        "      train_domain_loss.backward()  # backward pass: computes gradients\n",
        "\n",
        "      # training model using test data\n",
        "      data_test = data_test_iter.next()\n",
        "      test_img, _ = data_test\n",
        "\n",
        "      batch_size = len(test_img)\n",
        "\n",
        "      input_img = torch.FloatTensor(batch_size, 3, 224, 224)\n",
        "      domain_label = torch.ones(batch_size)\n",
        "      domain_label = domain_label.long()\n",
        "\n",
        "      test_img =  test_img.to(DEVICE)\n",
        "      input_img = input_img.to(DEVICE)\n",
        "      domain_label = domain_label.to(DEVICE)\n",
        "\n",
        "      input_img.resize_as_(test_img).copy_(test_img)\n",
        "\n",
        "      test_domain_output = net(input_img, alpha=ALPHA)\n",
        "      test_domain_loss = criterion(test_domain_output, domain_label)\n",
        "\n",
        "      running_test_domain_losses += test_domain_loss.item() * test_img.size(0)\n",
        "\n",
        "      # Compute gradients for each layer and update weights\n",
        "      test_domain_loss.backward()  # backward pass: computes gradients\n",
        "\n",
        "      #err = train_class_loss + train_domain_loss + test_domain_loss\n",
        "      #err.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      i += 1\n",
        "\n",
        "      print ('epoch: %d, [iter: %d / all %d], train_class_loss: %f, train_domain_loss: %f, test_domain_loss: %f' \\\n",
        "                % (epoch, i, len_dataloader, train_class_loss.data.cpu().numpy(),\n",
        "                  train_domain_loss.data.cpu().numpy(), test_domain_loss.data.cpu().item()))\n",
        "    \n",
        "\n",
        "    epoch_train_class_loss = running_train_class_losses / float(len_dataloader * BATCH_SIZE)\n",
        "    epoch_train_domain_loss = running_train_domain_losses / float(len_dataloader * BATCH_SIZE)\n",
        "    epoch_test_domain_loss = running_test_domain_losses / float(len_dataloader * BATCH_SIZE)\n",
        "\n",
        "    train_class_losses.append(epoch_train_class_loss)\n",
        "    train_domain_losses.append(epoch_train_domain_loss)\n",
        "    test_domain_losses.append(epoch_test_domain_loss)\n",
        "\n",
        "    print(\"Train class loss: {}\".format(epoch_train_class_loss))\n",
        "    print(\"Train domain loss: {}\".format(epoch_train_domain_loss))\n",
        "    print(\"Test domain loss: {}\".format(epoch_test_domain_loss))\n",
        "\n",
        "    #Step the scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "  time_elapsed = time.time() - since\n",
        "  print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmWvU7SKIc7C"
      },
      "source": [
        "**Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmD_4kRJIfSo"
      },
      "source": [
        "if VALIDATION:\n",
        "\n",
        "  since = time.time()\n",
        "\n",
        "  print(\"Current Hyperparameters:\")\n",
        "  print(f\"Epochs = {NUM_EPOCHS}\")\n",
        "  print(f\"  Scheduler:\")\n",
        "  print(f\"  - Step Size = {STEP_SIZE}\")\n",
        "  print(f\"  - Gamma = {GAMMA}\")\n",
        "  print(f\"Optimizer: {optimizer}\")\n",
        "  print(f\"Device: {DEVICE}\")\n",
        "  print()\n",
        "\n",
        "  train_loss = []\n",
        "  train_acc = []\n",
        "  val_loss = []\n",
        "  val_acc = []\n",
        "\n",
        "  # By default, everything is loaded to cpu\n",
        "  net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "\n",
        "  cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "  current_step = 0\n",
        "  # Start iterating over the epochs\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
        "\n",
        "    net.train() # Sets module in training mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    # Iterate over the dataset\n",
        "    for images, labels in train_dataloader:\n",
        "      # Bring data over the device of choice\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "\n",
        "      # PyTorch, by default, accumulates gradients after each backward pass\n",
        "      # We need to manually set the gradients to zero before starting a new iteration\n",
        "      optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "      with torch.set_grad_enabled(True):\n",
        "\n",
        "        # Forward pass to the network\n",
        "        outputs = net(images)\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        # Compute loss based on output and ground truth\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Log loss\n",
        "        if current_step % LOG_FREQUENCY == 0:\n",
        "          print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "        # Compute gradients for each layer and update weights\n",
        "        loss.backward()  # backward pass: computes gradients\n",
        "        optimizer.step() # update weights based on accumulated gradients\n",
        "        \n",
        "      current_step += 1\n",
        "\n",
        "      # statistics\n",
        "      running_loss += loss.item() * images.size(0)\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item() \n",
        "    \n",
        "    epoch_loss = running_loss / float(len(train_dataloader)*BATCH_SIZE)\n",
        "    epoch_acc = running_corrects / float(len(train_dataloader)*BATCH_SIZE)\n",
        "\n",
        "    train_loss.append(epoch_loss) # loss computed as the average on mini-batches\n",
        "    #train_loss.append(loss) # loss computed only on the last batch\n",
        "    train_acc.append(epoch_acc)\n",
        "\n",
        "    print('Train -> Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "\n",
        "    ### Start Validation phase\n",
        "\n",
        "    net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "    net.train(False) # Set Network to evaluation mode\n",
        "\n",
        "    running_val_loss = 0.0\n",
        "    running_val_corrects = 0\n",
        "\n",
        "    for images, labels in val_dataloader:\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "\n",
        "      # zero the parameter gradients\n",
        "      #optimizer.zero_grad()\n",
        "\n",
        "      with torch.set_grad_enabled(False):\n",
        "\n",
        "        # Forward Pass\n",
        "        outputs = net(images)\n",
        "\n",
        "        # Get predictions\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        # Update Corrects\n",
        "        #running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        # Calculate Loss\n",
        "        loss = criterion(outputs.data, labels)\n",
        "\n",
        "      # Update Corrects and Loss\n",
        "      running_val_loss += loss.item() * images.size(0)\n",
        "      running_val_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "    epoch_val_loss = running_val_loss / float(len(val_dataset))\n",
        "    epoch_val_acc = running_val_corrects / float(len(val_dataset))\n",
        "\n",
        "    val_loss.append(epoch_val_loss)\n",
        "    val_acc.append(epoch_val_acc)\n",
        "\n",
        "    print('Validation -> Loss: {:.4f} Acc: {:.4f}'.format(epoch_val_loss, epoch_val_acc))\n",
        "\n",
        "    # Calculate Accuracy\n",
        "    accuracy = running_val_corrects / float(len(val_dataset))\n",
        "\n",
        "    print('Validation Accuracy: {}'.format(accuracy))\n",
        "\n",
        "    # Step the scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    print()\n",
        "\n",
        "  time_elapsed = time.time() - since\n",
        "  print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIBgA5mIXTin"
      },
      "source": [
        "**Validation DANN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML8YMg2UXVFe"
      },
      "source": [
        "#net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "#net.train(False) # Set Network to evaluation mode\n",
        "\n",
        "def test(dataloader, epoch, net, train):\n",
        "\n",
        "  net = net.to(DEVICE)\n",
        "\n",
        "  net.eval() # Set Network to evaluation mode\n",
        "\n",
        "  len_dataloader = len(dataloader)\n",
        "  data_test_iter = iter(dataloader)\n",
        "\n",
        "  i = 0\n",
        "  running_corrects = 0\n",
        "  running_loss = 0.0\n",
        "  total_dim = 0\n",
        "\n",
        "  while i < len_dataloader:\n",
        "\n",
        "   # test model using target data\n",
        "    data_test = data_test_iter.next()\n",
        "    test_img, test_label = data_test\n",
        "\n",
        "    batch_size = len(test_label)\n",
        "\n",
        "    input_img = torch.FloatTensor(batch_size, 3, 224, 224)\n",
        "    class_label = torch.LongTensor(batch_size)\n",
        "\n",
        "    test_img = test_img.to(DEVICE)\n",
        "    test_label = test_label.to(DEVICE)\n",
        "    input_img = input_img.to(DEVICE)\n",
        "    class_label = class_label.to(DEVICE)\n",
        "\n",
        "    input_img.resize_as_(test_img).copy_(test_img)\n",
        "    class_label.resize_as_(test_label).copy_(test_label)\n",
        "\n",
        "    class_output = net(input_img)\n",
        "    loss = criterion(class_output, class_label)\n",
        "    pred = class_output.data.max(1, keepdim=True)[1]\n",
        "    #_, pred = torch.max(class_output.data, 1)\n",
        "    #running_corrects += torch.sum(pred == class_label.data).data.item()\n",
        "    running_loss += loss.item() * test_img.size(0)\n",
        "    running_corrects += pred.eq(class_label.data.view_as(pred)).cpu().sum()\n",
        "    total_dim += batch_size\n",
        "\n",
        "    i += 1\n",
        "\n",
        "  accuracy = running_corrects.data.numpy() * 1.0 / total_dim\n",
        "  final_loss = running_loss / float(total_dim)\n",
        "  #accuracy = running_corrects / float(len(test_dataset))\n",
        "  \n",
        "  if train == 1:\n",
        "    print('Validation Accuracy: {}'.format(accuracy))\n",
        "    print('Validation Loss: {}'.format(final_loss))\n",
        "    print('epoch: %d, accuracy on the validation dataset: %f' % (epoch, accuracy))\n",
        "    print('epoch: %d, loss on the validation dataset: %f' % (epoch, final_loss))\n",
        "  else:\n",
        "    print('Training Accuracy: {}'.format(accuracy))\n",
        "    print('Training Loss: {}'.format(final_loss))\n",
        "    print('epoch: %d, accuracy on the training dataset: %f' % (epoch, accuracy))\n",
        "    print('epoch: %d, loss on the training dataset: %f' % (epoch, final_loss))\n",
        "  \n",
        "\n",
        "  return accuracy, final_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtkDVQicXTAM",
        "outputId": "366bddcf-092f-4ee2-8f24-8e5139e957f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if VALIDATION_DANN:\n",
        "\n",
        "  since = time.time()\n",
        "\n",
        "  print(\"Current Hyperparameters:\")\n",
        "  print(f\"Epochs = {NUM_EPOCHS}\")\n",
        "  print(f\"  Scheduler:\")\n",
        "  print(f\"  - Step Size = {STEP_SIZE}\")\n",
        "  print(f\"  - Gamma = {GAMMA}\")\n",
        "  print(f\"Optimizer: {optimizer}\")\n",
        "  print(f\"Device: {DEVICE}\")\n",
        "  print(f\"Alpha: {ALPHA}\")\n",
        "  print()\n",
        "\n",
        "  train_class_losses = []\n",
        "  train_domain_losses = []\n",
        "  val_domain_losses = []\n",
        "  train_acc = []\n",
        "  val_acc = []\n",
        "  train_loss = []\n",
        "  val_loss = []\n",
        "  tot_losses = []\n",
        "\n",
        "  # By default, everything is loaded to cpu\n",
        "  net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "\n",
        "  cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "  current_step = 0\n",
        "  # Start iterating over the epochs\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
        "\n",
        "    len_dataloader = min(len(train_dataloader), len(val_dataloader))\n",
        "    data_train_iter = iter(train_dataloader)\n",
        "    data_val_iter = iter(val_dataloader)\n",
        "\n",
        "    running_train_class_losses = 0\n",
        "    running_train_domain_losses = 0\n",
        "    running_val_domain_losses = 0\n",
        "    running_tot_loss = 0\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    while i < len_dataloader:\n",
        "      \n",
        "      if FIX_ALPHA == False:\n",
        "        p = float(i + epoch * len_dataloader) / NUM_EPOCHS / len_dataloader\n",
        "        ALPHA = 2. / (1. + np.exp(-10 * p)) - 1\n",
        "        print(ALPHA)\n",
        "\n",
        "      # training model using source data\n",
        "      data_train = data_train_iter.next()\n",
        "      train_img, train_label = data_train\n",
        "\n",
        "      #net.zero_grad()\n",
        "      optimizer.zero_grad()\n",
        "      batch_size = len(train_label)\n",
        "\n",
        "      input_img = torch.FloatTensor(batch_size, 3, 224, 224)\n",
        "      class_label = torch.LongTensor(batch_size)\n",
        "      domain_label = torch.zeros(batch_size)\n",
        "      domain_label = domain_label.long()\n",
        "\n",
        "      train_img = train_img.to(DEVICE)\n",
        "      train_label = train_label.to(DEVICE)\n",
        "      input_img = input_img.to(DEVICE)\n",
        "      class_label = class_label.to(DEVICE)\n",
        "      domain_label = domain_label.to(DEVICE)\n",
        "\n",
        "      input_img.resize_as_(train_img).copy_(train_img)\n",
        "      class_label.resize_as_(train_label).copy_(train_label)\n",
        "\n",
        "      train_class_output = net(input_img)\n",
        "      train_class_loss = criterion(train_class_output, class_label)\n",
        "\n",
        "      running_train_class_losses += train_class_loss.item() * train_img.size(0)\n",
        "\n",
        "      # Compute gradients for each layer and update weights\n",
        "      train_class_loss.backward()  # backward pass: computes gradients\n",
        "\n",
        "      train_domain_output = net(input_img, alpha=ALPHA)\n",
        "      train_domain_loss = criterion(train_domain_output, domain_label)\n",
        "\n",
        "      running_train_domain_losses += train_domain_loss.item() * train_img.size(0)\n",
        "\n",
        "      # Compute gradients for each layer and update weights\n",
        "      train_domain_loss.backward()  # backward pass: computes gradients\n",
        "\n",
        "      # training model using test data\n",
        "      data_val = data_val_iter.next()\n",
        "      val_img, _ = data_val\n",
        "\n",
        "      batch_size = len(val_img)\n",
        "\n",
        "      input_img = torch.FloatTensor(batch_size, 3, 224, 224)\n",
        "      domain_label = torch.ones(batch_size)\n",
        "      domain_label = domain_label.long()\n",
        "\n",
        "      val_img =  val_img.to(DEVICE)\n",
        "      input_img = input_img.to(DEVICE)\n",
        "      domain_label = domain_label.to(DEVICE)\n",
        "\n",
        "      input_img.resize_as_(val_img).copy_(val_img)\n",
        "\n",
        "      val_domain_output = net(input_img, alpha=ALPHA)\n",
        "      val_domain_loss = criterion(val_domain_output, domain_label)\n",
        "\n",
        "      running_val_domain_losses += val_domain_loss.item() * val_img.size(0)\n",
        "\n",
        "      # Compute gradients for each layer and update weights\n",
        "      val_domain_loss.backward()  # backward pass: computes gradients\n",
        "\n",
        "      running_tot_loss += train_class_loss.item() * train_img.size(0)\n",
        "      running_tot_loss += train_domain_loss.item() * train_img.size(0)\n",
        "      running_tot_loss += val_domain_loss.item() * train_img.size(0)\n",
        "      #err.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      i += 1\n",
        "\n",
        "      print ('epoch: %d, [iter: %d / all %d], train_class_loss: %f, train_domain_loss: %f, test_domain_loss: %f' \\\n",
        "                % (epoch, i, len_dataloader, train_class_loss.data.cpu().numpy(),\n",
        "                  train_domain_loss.data.cpu().numpy(), val_domain_loss.data.cpu().item()))\n",
        "    \n",
        "\n",
        "    epoch_train_class_loss = running_train_class_losses / float(len_dataloader * BATCH_SIZE)\n",
        "    epoch_train_domain_loss = running_train_domain_losses / float(len_dataloader * BATCH_SIZE)\n",
        "    epoch_val_domain_loss = running_val_domain_losses / float(len_dataloader * BATCH_SIZE)\n",
        "    epoch_tot_loss = running_tot_loss / float(len_dataloader * BATCH_SIZE)\n",
        "\n",
        "    train_class_losses.append(epoch_train_class_loss)\n",
        "    train_domain_losses.append(epoch_train_domain_loss)\n",
        "    val_domain_losses.append(epoch_val_domain_loss)\n",
        "    tot_losses.append(epoch_tot_loss)\n",
        "\n",
        "    print(\"Train class loss: {}\".format(epoch_train_class_loss))\n",
        "    print(\"Train domain loss: {}\".format(epoch_train_domain_loss))\n",
        "    print(\"Val domain loss: {}\".format(epoch_val_domain_loss))\n",
        "    print(\"Total loss: {}\".format(epoch_tot_loss))\n",
        "\n",
        "    acc_t, loss_t = test(train_dataloader, epoch, net, 0)\n",
        "    acc_v, loss_v = test(val_dataloader, epoch, net, 1)\n",
        "\n",
        "    train_acc.append(acc_t)\n",
        "    val_acc.append(acc_v)\n",
        "    train_loss.append(loss_t)\n",
        "    val_loss.append(loss_v)\n",
        "\n",
        "    #Step the scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    print()\n",
        "\n",
        "  time_elapsed = time.time() - since\n",
        "  print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current Hyperparameters:\n",
            "Epochs = 30\n",
            "  Scheduler:\n",
            "  - Step Size = 20\n",
            "  - Gamma = 0.1\n",
            "Optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    initial_lr: 0.0001\n",
            "    lr: 0.0001\n",
            "    momentum: 0.9\n",
            "    nesterov: False\n",
            "    weight_decay: 5e-05\n",
            ")\n",
            "Device: cuda\n",
            "Alpha: 0.5\n",
            "\n",
            "Starting epoch 1/30, LR = [0.0001]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, [iter: 1 / all 26], train_class_loss: 1.967592, train_domain_loss: 0.903551, test_domain_loss: 0.679992\n",
            "epoch: 0, [iter: 2 / all 26], train_class_loss: 1.979210, train_domain_loss: 0.839771, test_domain_loss: 0.681546\n",
            "epoch: 0, [iter: 3 / all 26], train_class_loss: 1.983134, train_domain_loss: 0.842197, test_domain_loss: 0.704661\n",
            "epoch: 0, [iter: 4 / all 26], train_class_loss: 1.935264, train_domain_loss: 0.727685, test_domain_loss: 0.713260\n",
            "epoch: 0, [iter: 5 / all 26], train_class_loss: 1.941480, train_domain_loss: 0.722029, test_domain_loss: 0.716929\n",
            "epoch: 0, [iter: 6 / all 26], train_class_loss: 1.954481, train_domain_loss: 0.590642, test_domain_loss: 0.684076\n",
            "epoch: 0, [iter: 7 / all 26], train_class_loss: 1.913856, train_domain_loss: 0.529760, test_domain_loss: 0.727649\n",
            "epoch: 0, [iter: 8 / all 26], train_class_loss: 1.940026, train_domain_loss: 0.482370, test_domain_loss: 0.726601\n",
            "epoch: 0, [iter: 9 / all 26], train_class_loss: 1.942278, train_domain_loss: 0.402101, test_domain_loss: 0.722072\n",
            "epoch: 0, [iter: 10 / all 26], train_class_loss: 1.868517, train_domain_loss: 0.399674, test_domain_loss: 0.727073\n",
            "epoch: 0, [iter: 11 / all 26], train_class_loss: 1.874294, train_domain_loss: 0.377464, test_domain_loss: 0.651771\n",
            "epoch: 0, [iter: 12 / all 26], train_class_loss: 1.847043, train_domain_loss: 0.403049, test_domain_loss: 0.608740\n",
            "epoch: 0, [iter: 13 / all 26], train_class_loss: 1.834369, train_domain_loss: 0.329080, test_domain_loss: 0.659779\n",
            "epoch: 0, [iter: 14 / all 26], train_class_loss: 1.830994, train_domain_loss: 0.333310, test_domain_loss: 0.576461\n",
            "epoch: 0, [iter: 15 / all 26], train_class_loss: 1.775305, train_domain_loss: 0.319396, test_domain_loss: 0.449593\n",
            "epoch: 0, [iter: 16 / all 26], train_class_loss: 1.890997, train_domain_loss: 0.312871, test_domain_loss: 0.431306\n",
            "epoch: 0, [iter: 17 / all 26], train_class_loss: 1.807607, train_domain_loss: 0.314314, test_domain_loss: 0.380286\n",
            "epoch: 0, [iter: 18 / all 26], train_class_loss: 1.772888, train_domain_loss: 0.305680, test_domain_loss: 0.317729\n",
            "epoch: 0, [iter: 19 / all 26], train_class_loss: 1.776536, train_domain_loss: 0.329833, test_domain_loss: 0.288076\n",
            "epoch: 0, [iter: 20 / all 26], train_class_loss: 1.820174, train_domain_loss: 0.322631, test_domain_loss: 0.238623\n",
            "epoch: 0, [iter: 21 / all 26], train_class_loss: 1.824638, train_domain_loss: 0.312261, test_domain_loss: 0.240836\n",
            "epoch: 0, [iter: 22 / all 26], train_class_loss: 1.759106, train_domain_loss: 0.240598, test_domain_loss: 0.201510\n",
            "epoch: 0, [iter: 23 / all 26], train_class_loss: 1.757413, train_domain_loss: 0.264109, test_domain_loss: 0.224053\n",
            "epoch: 0, [iter: 24 / all 26], train_class_loss: 1.684700, train_domain_loss: 0.244004, test_domain_loss: 0.256456\n",
            "epoch: 0, [iter: 25 / all 26], train_class_loss: 1.820418, train_domain_loss: 0.210871, test_domain_loss: 0.404977\n",
            "epoch: 0, [iter: 26 / all 26], train_class_loss: 1.621684, train_domain_loss: 0.200964, test_domain_loss: 0.251244\n",
            "Train class loss: 1.8509231805801392\n",
            "Train domain loss: 0.4330851613329007\n",
            "Val domain loss: 0.5102037718662848\n",
            "Total loss: 2.7942121137793245\n",
            "Training Accuracy: 0.5192307692307693\n",
            "Training Loss: 1.5917618503937354\n",
            "epoch: 0, accuracy on the training dataset: 0.519231\n",
            "epoch: 0, loss on the training dataset: 1.591762\n",
            "Validation Accuracy: 0.15169254263171292\n",
            "Validation Loss: 1.9657900516056597\n",
            "epoch: 0, accuracy on the validation dataset: 0.151693\n",
            "epoch: 0, loss on the validation dataset: 1.965790\n",
            "\n",
            "Starting epoch 2/30, LR = [0.0001]\n",
            "epoch: 1, [iter: 1 / all 26], train_class_loss: 1.555880, train_domain_loss: 0.204994, test_domain_loss: 0.238895\n",
            "epoch: 1, [iter: 2 / all 26], train_class_loss: 1.712124, train_domain_loss: 0.220414, test_domain_loss: 0.194500\n",
            "epoch: 1, [iter: 3 / all 26], train_class_loss: 1.747425, train_domain_loss: 0.200894, test_domain_loss: 0.207108\n",
            "epoch: 1, [iter: 4 / all 26], train_class_loss: 1.698829, train_domain_loss: 0.170890, test_domain_loss: 0.178588\n",
            "epoch: 1, [iter: 5 / all 26], train_class_loss: 1.651733, train_domain_loss: 0.146862, test_domain_loss: 0.173848\n",
            "epoch: 1, [iter: 6 / all 26], train_class_loss: 1.700838, train_domain_loss: 0.150175, test_domain_loss: 0.180516\n",
            "epoch: 1, [iter: 7 / all 26], train_class_loss: 1.576591, train_domain_loss: 0.152582, test_domain_loss: 0.179206\n",
            "epoch: 1, [iter: 8 / all 26], train_class_loss: 1.567835, train_domain_loss: 0.131121, test_domain_loss: 0.157600\n",
            "epoch: 1, [iter: 9 / all 26], train_class_loss: 1.513917, train_domain_loss: 0.136370, test_domain_loss: 0.159373\n",
            "epoch: 1, [iter: 10 / all 26], train_class_loss: 1.493406, train_domain_loss: 0.111839, test_domain_loss: 0.178430\n",
            "epoch: 1, [iter: 11 / all 26], train_class_loss: 1.530426, train_domain_loss: 0.133823, test_domain_loss: 0.137536\n",
            "epoch: 1, [iter: 12 / all 26], train_class_loss: 1.400793, train_domain_loss: 0.119582, test_domain_loss: 0.143093\n",
            "epoch: 1, [iter: 13 / all 26], train_class_loss: 1.462824, train_domain_loss: 0.103924, test_domain_loss: 0.171837\n",
            "epoch: 1, [iter: 14 / all 26], train_class_loss: 1.527684, train_domain_loss: 0.097826, test_domain_loss: 0.130245\n",
            "epoch: 1, [iter: 15 / all 26], train_class_loss: 1.622181, train_domain_loss: 0.092833, test_domain_loss: 0.117186\n",
            "epoch: 1, [iter: 16 / all 26], train_class_loss: 1.425811, train_domain_loss: 0.116966, test_domain_loss: 0.121755\n",
            "epoch: 1, [iter: 17 / all 26], train_class_loss: 1.418160, train_domain_loss: 0.074169, test_domain_loss: 0.125212\n",
            "epoch: 1, [iter: 18 / all 26], train_class_loss: 1.449248, train_domain_loss: 0.092142, test_domain_loss: 0.108286\n",
            "epoch: 1, [iter: 19 / all 26], train_class_loss: 1.378072, train_domain_loss: 0.080811, test_domain_loss: 0.117159\n",
            "epoch: 1, [iter: 20 / all 26], train_class_loss: 1.319772, train_domain_loss: 0.070646, test_domain_loss: 0.115634\n",
            "epoch: 1, [iter: 21 / all 26], train_class_loss: 1.361728, train_domain_loss: 0.072445, test_domain_loss: 0.099980\n",
            "epoch: 1, [iter: 22 / all 26], train_class_loss: 1.375454, train_domain_loss: 0.064975, test_domain_loss: 0.088379\n",
            "epoch: 1, [iter: 23 / all 26], train_class_loss: 1.430895, train_domain_loss: 0.106790, test_domain_loss: 0.114796\n",
            "epoch: 1, [iter: 24 / all 26], train_class_loss: 1.330714, train_domain_loss: 0.080832, test_domain_loss: 0.175815\n",
            "epoch: 1, [iter: 25 / all 26], train_class_loss: 1.306297, train_domain_loss: 0.075933, test_domain_loss: 0.288561\n",
            "epoch: 1, [iter: 26 / all 26], train_class_loss: 1.289234, train_domain_loss: 0.064240, test_domain_loss: 0.165649\n",
            "Train class loss: 1.4941488733658423\n",
            "Train domain loss: 0.11823367298795627\n",
            "Val domain loss: 0.15650723244135195\n",
            "Total loss: 1.7688897787951505\n",
            "Training Accuracy: 0.7626201923076923\n",
            "Training Loss: 1.2247744661111097\n",
            "epoch: 1, accuracy on the training dataset: 0.762620\n",
            "epoch: 1, loss on the training dataset: 1.224774\n",
            "Validation Accuracy: 0.255790277424281\n",
            "Validation Loss: 1.9149558967110702\n",
            "epoch: 1, accuracy on the validation dataset: 0.255790\n",
            "epoch: 1, loss on the validation dataset: 1.914956\n",
            "\n",
            "Starting epoch 3/30, LR = [0.0001]\n",
            "epoch: 2, [iter: 1 / all 26], train_class_loss: 1.224562, train_domain_loss: 0.059205, test_domain_loss: 0.170667\n",
            "epoch: 2, [iter: 2 / all 26], train_class_loss: 1.336263, train_domain_loss: 0.083080, test_domain_loss: 0.112880\n",
            "epoch: 2, [iter: 3 / all 26], train_class_loss: 1.412197, train_domain_loss: 0.081675, test_domain_loss: 0.111429\n",
            "epoch: 2, [iter: 4 / all 26], train_class_loss: 1.381206, train_domain_loss: 0.099204, test_domain_loss: 0.111780\n",
            "epoch: 2, [iter: 5 / all 26], train_class_loss: 1.392173, train_domain_loss: 0.074046, test_domain_loss: 0.101552\n",
            "epoch: 2, [iter: 6 / all 26], train_class_loss: 1.400266, train_domain_loss: 0.110190, test_domain_loss: 0.111496\n",
            "epoch: 2, [iter: 7 / all 26], train_class_loss: 1.273468, train_domain_loss: 0.078048, test_domain_loss: 0.091716\n",
            "epoch: 2, [iter: 8 / all 26], train_class_loss: 1.200374, train_domain_loss: 0.074803, test_domain_loss: 0.093081\n",
            "epoch: 2, [iter: 9 / all 26], train_class_loss: 1.203732, train_domain_loss: 0.078498, test_domain_loss: 0.098677\n",
            "epoch: 2, [iter: 10 / all 26], train_class_loss: 1.240718, train_domain_loss: 0.059021, test_domain_loss: 0.110163\n",
            "epoch: 2, [iter: 11 / all 26], train_class_loss: 1.274199, train_domain_loss: 0.070398, test_domain_loss: 0.096718\n",
            "epoch: 2, [iter: 12 / all 26], train_class_loss: 1.159148, train_domain_loss: 0.072682, test_domain_loss: 0.080048\n",
            "epoch: 2, [iter: 13 / all 26], train_class_loss: 1.352882, train_domain_loss: 0.085180, test_domain_loss: 0.111428\n",
            "epoch: 2, [iter: 14 / all 26], train_class_loss: 1.119139, train_domain_loss: 0.065573, test_domain_loss: 0.076326\n",
            "epoch: 2, [iter: 15 / all 26], train_class_loss: 0.987040, train_domain_loss: 0.060293, test_domain_loss: 0.058833\n",
            "epoch: 2, [iter: 16 / all 26], train_class_loss: 1.111581, train_domain_loss: 0.079990, test_domain_loss: 0.073811\n",
            "epoch: 2, [iter: 17 / all 26], train_class_loss: 1.210039, train_domain_loss: 0.056291, test_domain_loss: 0.075650\n",
            "epoch: 2, [iter: 18 / all 26], train_class_loss: 1.008722, train_domain_loss: 0.042289, test_domain_loss: 0.076676\n",
            "epoch: 2, [iter: 19 / all 26], train_class_loss: 1.151183, train_domain_loss: 0.071706, test_domain_loss: 0.071865\n",
            "epoch: 2, [iter: 20 / all 26], train_class_loss: 1.143396, train_domain_loss: 0.063461, test_domain_loss: 0.063487\n",
            "epoch: 2, [iter: 21 / all 26], train_class_loss: 1.221881, train_domain_loss: 0.067490, test_domain_loss: 0.077807\n",
            "epoch: 2, [iter: 22 / all 26], train_class_loss: 1.112881, train_domain_loss: 0.046757, test_domain_loss: 0.071100\n",
            "epoch: 2, [iter: 23 / all 26], train_class_loss: 1.086534, train_domain_loss: 0.054716, test_domain_loss: 0.085610\n",
            "epoch: 2, [iter: 24 / all 26], train_class_loss: 1.144541, train_domain_loss: 0.041660, test_domain_loss: 0.134216\n",
            "epoch: 2, [iter: 25 / all 26], train_class_loss: 1.129434, train_domain_loss: 0.048403, test_domain_loss: 0.184487\n",
            "epoch: 2, [iter: 26 / all 26], train_class_loss: 0.962894, train_domain_loss: 0.041098, test_domain_loss: 0.127414\n",
            "Train class loss: 1.201555772469594\n",
            "Train domain loss: 0.0679136706659427\n",
            "Val domain loss: 0.09918910007064159\n",
            "Total loss: 1.3686585432061782\n",
            "Training Accuracy: 0.8419471153846154\n",
            "Training Loss: 0.9787753705794995\n",
            "epoch: 2, accuracy on the training dataset: 0.841947\n",
            "epoch: 2, loss on the training dataset: 0.978775\n",
            "Validation Accuracy: 0.2970221430389412\n",
            "Validation Loss: 1.8780717543895447\n",
            "epoch: 2, accuracy on the validation dataset: 0.297022\n",
            "epoch: 2, loss on the validation dataset: 1.878072\n",
            "\n",
            "Starting epoch 4/30, LR = [0.0001]\n",
            "epoch: 3, [iter: 1 / all 26], train_class_loss: 1.150173, train_domain_loss: 0.048897, test_domain_loss: 0.127272\n",
            "epoch: 3, [iter: 2 / all 26], train_class_loss: 0.990544, train_domain_loss: 0.051753, test_domain_loss: 0.087687\n",
            "epoch: 3, [iter: 3 / all 26], train_class_loss: 1.001520, train_domain_loss: 0.050825, test_domain_loss: 0.079447\n",
            "epoch: 3, [iter: 4 / all 26], train_class_loss: 1.174612, train_domain_loss: 0.057983, test_domain_loss: 0.090828\n",
            "epoch: 3, [iter: 5 / all 26], train_class_loss: 1.103134, train_domain_loss: 0.051384, test_domain_loss: 0.078250\n",
            "epoch: 3, [iter: 6 / all 26], train_class_loss: 0.865665, train_domain_loss: 0.036819, test_domain_loss: 0.092473\n",
            "epoch: 3, [iter: 7 / all 26], train_class_loss: 1.011508, train_domain_loss: 0.057852, test_domain_loss: 0.082606\n",
            "epoch: 3, [iter: 8 / all 26], train_class_loss: 0.991455, train_domain_loss: 0.045457, test_domain_loss: 0.084482\n",
            "epoch: 3, [iter: 9 / all 26], train_class_loss: 1.065163, train_domain_loss: 0.045303, test_domain_loss: 0.085109\n",
            "epoch: 3, [iter: 10 / all 26], train_class_loss: 0.935303, train_domain_loss: 0.039326, test_domain_loss: 0.101517\n",
            "epoch: 3, [iter: 11 / all 26], train_class_loss: 1.093056, train_domain_loss: 0.044170, test_domain_loss: 0.072835\n",
            "epoch: 3, [iter: 12 / all 26], train_class_loss: 1.083512, train_domain_loss: 0.065282, test_domain_loss: 0.075702\n",
            "epoch: 3, [iter: 13 / all 26], train_class_loss: 0.885728, train_domain_loss: 0.033615, test_domain_loss: 0.082649\n",
            "epoch: 3, [iter: 14 / all 26], train_class_loss: 0.873115, train_domain_loss: 0.038036, test_domain_loss: 0.064629\n",
            "epoch: 3, [iter: 15 / all 26], train_class_loss: 0.847920, train_domain_loss: 0.057896, test_domain_loss: 0.057187\n",
            "epoch: 3, [iter: 16 / all 26], train_class_loss: 1.025274, train_domain_loss: 0.037508, test_domain_loss: 0.063021\n",
            "epoch: 3, [iter: 17 / all 26], train_class_loss: 1.031779, train_domain_loss: 0.048527, test_domain_loss: 0.061674\n",
            "epoch: 3, [iter: 18 / all 26], train_class_loss: 0.879978, train_domain_loss: 0.043827, test_domain_loss: 0.060158\n",
            "epoch: 3, [iter: 19 / all 26], train_class_loss: 1.050582, train_domain_loss: 0.064020, test_domain_loss: 0.065493\n",
            "epoch: 3, [iter: 20 / all 26], train_class_loss: 0.867979, train_domain_loss: 0.048468, test_domain_loss: 0.059445\n",
            "epoch: 3, [iter: 21 / all 26], train_class_loss: 0.924946, train_domain_loss: 0.072181, test_domain_loss: 0.059958\n",
            "epoch: 3, [iter: 22 / all 26], train_class_loss: 0.851157, train_domain_loss: 0.065915, test_domain_loss: 0.056223\n",
            "epoch: 3, [iter: 23 / all 26], train_class_loss: 1.014227, train_domain_loss: 0.041377, test_domain_loss: 0.071350\n",
            "epoch: 3, [iter: 24 / all 26], train_class_loss: 1.103083, train_domain_loss: 0.074409, test_domain_loss: 0.093978\n",
            "epoch: 3, [iter: 25 / all 26], train_class_loss: 0.940777, train_domain_loss: 0.033753, test_domain_loss: 0.131279\n",
            "epoch: 3, [iter: 26 / all 26], train_class_loss: 0.876245, train_domain_loss: 0.047585, test_domain_loss: 0.101421\n",
            "Train class loss: 0.9860936953471258\n",
            "Train domain loss: 0.050083399105530516\n",
            "Val domain loss: 0.0802566479318417\n",
            "Total loss: 1.1164337423844979\n",
            "Training Accuracy: 0.8822115384615384\n",
            "Training Loss: 0.8034213827206538\n",
            "epoch: 3, accuracy on the training dataset: 0.882212\n",
            "epoch: 3, loss on the training dataset: 0.803421\n",
            "Validation Accuracy: 0.24382794604224994\n",
            "Validation Loss: 1.843792897261078\n",
            "epoch: 3, accuracy on the validation dataset: 0.243828\n",
            "epoch: 3, loss on the validation dataset: 1.843793\n",
            "\n",
            "Starting epoch 5/30, LR = [0.0001]\n",
            "epoch: 4, [iter: 1 / all 26], train_class_loss: 1.018663, train_domain_loss: 0.048764, test_domain_loss: 0.083194\n",
            "epoch: 4, [iter: 2 / all 26], train_class_loss: 0.847064, train_domain_loss: 0.027045, test_domain_loss: 0.077694\n",
            "epoch: 4, [iter: 3 / all 26], train_class_loss: 0.799504, train_domain_loss: 0.030886, test_domain_loss: 0.076407\n",
            "epoch: 4, [iter: 4 / all 26], train_class_loss: 0.917148, train_domain_loss: 0.038314, test_domain_loss: 0.079248\n",
            "epoch: 4, [iter: 5 / all 26], train_class_loss: 1.014825, train_domain_loss: 0.048197, test_domain_loss: 0.070550\n",
            "epoch: 4, [iter: 6 / all 26], train_class_loss: 1.009138, train_domain_loss: 0.050614, test_domain_loss: 0.070983\n",
            "epoch: 4, [iter: 7 / all 26], train_class_loss: 0.819853, train_domain_loss: 0.031506, test_domain_loss: 0.075986\n",
            "epoch: 4, [iter: 8 / all 26], train_class_loss: 0.779251, train_domain_loss: 0.039047, test_domain_loss: 0.065988\n",
            "epoch: 4, [iter: 9 / all 26], train_class_loss: 0.720334, train_domain_loss: 0.049299, test_domain_loss: 0.076186\n",
            "epoch: 4, [iter: 10 / all 26], train_class_loss: 0.823329, train_domain_loss: 0.045206, test_domain_loss: 0.081577\n",
            "epoch: 4, [iter: 11 / all 26], train_class_loss: 0.691177, train_domain_loss: 0.021202, test_domain_loss: 0.062834\n",
            "epoch: 4, [iter: 12 / all 26], train_class_loss: 0.892066, train_domain_loss: 0.026900, test_domain_loss: 0.059344\n",
            "epoch: 4, [iter: 13 / all 26], train_class_loss: 0.760485, train_domain_loss: 0.026168, test_domain_loss: 0.067401\n",
            "epoch: 4, [iter: 14 / all 26], train_class_loss: 0.869162, train_domain_loss: 0.056419, test_domain_loss: 0.061414\n",
            "epoch: 4, [iter: 15 / all 26], train_class_loss: 0.821481, train_domain_loss: 0.047093, test_domain_loss: 0.059342\n",
            "epoch: 4, [iter: 16 / all 26], train_class_loss: 0.898296, train_domain_loss: 0.048481, test_domain_loss: 0.054546\n",
            "epoch: 4, [iter: 17 / all 26], train_class_loss: 0.878234, train_domain_loss: 0.023257, test_domain_loss: 0.058304\n",
            "epoch: 4, [iter: 18 / all 26], train_class_loss: 0.634645, train_domain_loss: 0.034826, test_domain_loss: 0.059170\n",
            "epoch: 4, [iter: 19 / all 26], train_class_loss: 0.882634, train_domain_loss: 0.031244, test_domain_loss: 0.061079\n",
            "epoch: 4, [iter: 20 / all 26], train_class_loss: 0.860645, train_domain_loss: 0.033722, test_domain_loss: 0.052849\n",
            "epoch: 4, [iter: 21 / all 26], train_class_loss: 0.773075, train_domain_loss: 0.029109, test_domain_loss: 0.059063\n",
            "epoch: 4, [iter: 22 / all 26], train_class_loss: 0.906874, train_domain_loss: 0.038473, test_domain_loss: 0.055557\n",
            "epoch: 4, [iter: 23 / all 26], train_class_loss: 0.727490, train_domain_loss: 0.040332, test_domain_loss: 0.074852\n",
            "epoch: 4, [iter: 24 / all 26], train_class_loss: 0.778256, train_domain_loss: 0.036387, test_domain_loss: 0.077567\n",
            "epoch: 4, [iter: 25 / all 26], train_class_loss: 0.722617, train_domain_loss: 0.026182, test_domain_loss: 0.103551\n",
            "epoch: 4, [iter: 26 / all 26], train_class_loss: 0.723866, train_domain_loss: 0.039296, test_domain_loss: 0.087705\n",
            "Train class loss: 0.8296197010920598\n",
            "Train domain loss: 0.037229505009376086\n",
            "Val domain loss: 0.06970730395271228\n",
            "Total loss: 0.9365565100541482\n",
            "Training Accuracy: 0.9014423076923077\n",
            "Training Loss: 0.658324617605943\n",
            "epoch: 4, accuracy on the training dataset: 0.901442\n",
            "epoch: 4, loss on the training dataset: 0.658325\n",
            "Validation Accuracy: 0.2537541359124459\n",
            "Validation Loss: 1.8146188796765024\n",
            "epoch: 4, accuracy on the validation dataset: 0.253754\n",
            "epoch: 4, loss on the validation dataset: 1.814619\n",
            "\n",
            "Starting epoch 6/30, LR = [0.0001]\n",
            "epoch: 5, [iter: 1 / all 26], train_class_loss: 0.659497, train_domain_loss: 0.023328, test_domain_loss: 0.078829\n",
            "epoch: 5, [iter: 2 / all 26], train_class_loss: 0.740835, train_domain_loss: 0.023109, test_domain_loss: 0.062909\n",
            "epoch: 5, [iter: 3 / all 26], train_class_loss: 0.594709, train_domain_loss: 0.021986, test_domain_loss: 0.065940\n",
            "epoch: 5, [iter: 4 / all 26], train_class_loss: 0.781995, train_domain_loss: 0.031676, test_domain_loss: 0.069294\n",
            "epoch: 5, [iter: 5 / all 26], train_class_loss: 0.729638, train_domain_loss: 0.043490, test_domain_loss: 0.056830\n",
            "epoch: 5, [iter: 6 / all 26], train_class_loss: 0.714976, train_domain_loss: 0.030099, test_domain_loss: 0.060105\n",
            "epoch: 5, [iter: 7 / all 26], train_class_loss: 0.671749, train_domain_loss: 0.032959, test_domain_loss: 0.062263\n",
            "epoch: 5, [iter: 8 / all 26], train_class_loss: 0.803813, train_domain_loss: 0.022727, test_domain_loss: 0.063202\n",
            "epoch: 5, [iter: 9 / all 26], train_class_loss: 0.843536, train_domain_loss: 0.027747, test_domain_loss: 0.079247\n",
            "epoch: 5, [iter: 10 / all 26], train_class_loss: 0.688115, train_domain_loss: 0.023248, test_domain_loss: 0.067744\n",
            "epoch: 5, [iter: 11 / all 26], train_class_loss: 0.741198, train_domain_loss: 0.034725, test_domain_loss: 0.059668\n",
            "epoch: 5, [iter: 12 / all 26], train_class_loss: 0.725003, train_domain_loss: 0.023884, test_domain_loss: 0.054019\n",
            "epoch: 5, [iter: 13 / all 26], train_class_loss: 0.610178, train_domain_loss: 0.051730, test_domain_loss: 0.063297\n",
            "epoch: 5, [iter: 14 / all 26], train_class_loss: 0.668592, train_domain_loss: 0.023543, test_domain_loss: 0.058325\n",
            "epoch: 5, [iter: 15 / all 26], train_class_loss: 0.682296, train_domain_loss: 0.039434, test_domain_loss: 0.049307\n",
            "epoch: 5, [iter: 16 / all 26], train_class_loss: 0.734338, train_domain_loss: 0.035267, test_domain_loss: 0.061553\n",
            "epoch: 5, [iter: 17 / all 26], train_class_loss: 0.647906, train_domain_loss: 0.025699, test_domain_loss: 0.055414\n",
            "epoch: 5, [iter: 18 / all 26], train_class_loss: 0.670111, train_domain_loss: 0.035304, test_domain_loss: 0.052895\n",
            "epoch: 5, [iter: 19 / all 26], train_class_loss: 0.615257, train_domain_loss: 0.025562, test_domain_loss: 0.062348\n",
            "epoch: 5, [iter: 20 / all 26], train_class_loss: 0.606878, train_domain_loss: 0.019416, test_domain_loss: 0.051284\n",
            "epoch: 5, [iter: 21 / all 26], train_class_loss: 0.755365, train_domain_loss: 0.046039, test_domain_loss: 0.059612\n",
            "epoch: 5, [iter: 22 / all 26], train_class_loss: 0.685176, train_domain_loss: 0.024902, test_domain_loss: 0.049443\n",
            "epoch: 5, [iter: 23 / all 26], train_class_loss: 0.769806, train_domain_loss: 0.023105, test_domain_loss: 0.068632\n",
            "epoch: 5, [iter: 24 / all 26], train_class_loss: 0.693818, train_domain_loss: 0.035949, test_domain_loss: 0.060644\n",
            "epoch: 5, [iter: 25 / all 26], train_class_loss: 0.662334, train_domain_loss: 0.021509, test_domain_loss: 0.069934\n",
            "epoch: 5, [iter: 26 / all 26], train_class_loss: 0.586212, train_domain_loss: 0.028230, test_domain_loss: 0.069545\n",
            "Train class loss: 0.6955126890769372\n",
            "Train domain loss: 0.029794867508686505\n",
            "Val domain loss: 0.062010858207941055\n",
            "Total loss: 0.7873184147935647\n",
            "Training Accuracy: 0.9128605769230769\n",
            "Training Loss: 0.5428676547912451\n",
            "epoch: 5, accuracy on the training dataset: 0.912861\n",
            "epoch: 5, loss on the training dataset: 0.542868\n",
            "Validation Accuracy: 0.2336472384830746\n",
            "Validation Loss: 1.7927176020897264\n",
            "epoch: 5, accuracy on the validation dataset: 0.233647\n",
            "epoch: 5, loss on the validation dataset: 1.792718\n",
            "\n",
            "Starting epoch 7/30, LR = [0.0001]\n",
            "epoch: 6, [iter: 1 / all 26], train_class_loss: 0.714432, train_domain_loss: 0.016816, test_domain_loss: 0.059913\n",
            "epoch: 6, [iter: 2 / all 26], train_class_loss: 0.683955, train_domain_loss: 0.025728, test_domain_loss: 0.067847\n",
            "epoch: 6, [iter: 3 / all 26], train_class_loss: 0.689925, train_domain_loss: 0.025340, test_domain_loss: 0.064086\n",
            "epoch: 6, [iter: 4 / all 26], train_class_loss: 0.575108, train_domain_loss: 0.029082, test_domain_loss: 0.060901\n",
            "epoch: 6, [iter: 5 / all 26], train_class_loss: 0.662648, train_domain_loss: 0.029510, test_domain_loss: 0.055732\n",
            "epoch: 6, [iter: 6 / all 26], train_class_loss: 0.681863, train_domain_loss: 0.050331, test_domain_loss: 0.060560\n",
            "epoch: 6, [iter: 7 / all 26], train_class_loss: 0.622943, train_domain_loss: 0.023551, test_domain_loss: 0.057921\n",
            "epoch: 6, [iter: 8 / all 26], train_class_loss: 0.632679, train_domain_loss: 0.030089, test_domain_loss: 0.057648\n",
            "epoch: 6, [iter: 9 / all 26], train_class_loss: 0.580993, train_domain_loss: 0.016596, test_domain_loss: 0.063456\n",
            "epoch: 6, [iter: 10 / all 26], train_class_loss: 0.710629, train_domain_loss: 0.026092, test_domain_loss: 0.062562\n",
            "epoch: 6, [iter: 11 / all 26], train_class_loss: 0.567789, train_domain_loss: 0.016336, test_domain_loss: 0.058585\n",
            "epoch: 6, [iter: 12 / all 26], train_class_loss: 0.555658, train_domain_loss: 0.027530, test_domain_loss: 0.047833\n",
            "epoch: 6, [iter: 13 / all 26], train_class_loss: 0.604832, train_domain_loss: 0.065790, test_domain_loss: 0.060263\n",
            "epoch: 6, [iter: 14 / all 26], train_class_loss: 0.632226, train_domain_loss: 0.013993, test_domain_loss: 0.064371\n",
            "epoch: 6, [iter: 15 / all 26], train_class_loss: 0.509492, train_domain_loss: 0.032519, test_domain_loss: 0.054066\n",
            "epoch: 6, [iter: 16 / all 26], train_class_loss: 0.520387, train_domain_loss: 0.017256, test_domain_loss: 0.055322\n",
            "epoch: 6, [iter: 17 / all 26], train_class_loss: 0.646880, train_domain_loss: 0.015275, test_domain_loss: 0.061285\n",
            "epoch: 6, [iter: 18 / all 26], train_class_loss: 0.580730, train_domain_loss: 0.020624, test_domain_loss: 0.054982\n",
            "epoch: 6, [iter: 19 / all 26], train_class_loss: 0.393303, train_domain_loss: 0.026460, test_domain_loss: 0.060414\n",
            "epoch: 6, [iter: 20 / all 26], train_class_loss: 0.668246, train_domain_loss: 0.041875, test_domain_loss: 0.049425\n",
            "epoch: 6, [iter: 21 / all 26], train_class_loss: 0.419985, train_domain_loss: 0.007971, test_domain_loss: 0.059390\n",
            "epoch: 6, [iter: 22 / all 26], train_class_loss: 0.673791, train_domain_loss: 0.035773, test_domain_loss: 0.049841\n",
            "epoch: 6, [iter: 23 / all 26], train_class_loss: 0.584568, train_domain_loss: 0.014328, test_domain_loss: 0.062399\n",
            "epoch: 6, [iter: 24 / all 26], train_class_loss: 0.583531, train_domain_loss: 0.038378, test_domain_loss: 0.060427\n",
            "epoch: 6, [iter: 25 / all 26], train_class_loss: 0.481917, train_domain_loss: 0.031075, test_domain_loss: 0.068051\n",
            "epoch: 6, [iter: 26 / all 26], train_class_loss: 0.519150, train_domain_loss: 0.022411, test_domain_loss: 0.070504\n",
            "Train class loss: 0.5960637285159185\n",
            "Train domain loss: 0.026951029825095948\n",
            "Val domain loss: 0.0595300835199081\n",
            "Total loss: 0.6825448418609225\n",
            "Training Accuracy: 0.9170673076923077\n",
            "Training Loss: 0.45133639069703907\n",
            "epoch: 6, accuracy on the training dataset: 0.917067\n",
            "epoch: 6, loss on the training dataset: 0.451336\n",
            "Validation Accuracy: 0.26724357342835325\n",
            "Validation Loss: 1.776413936110118\n",
            "epoch: 6, accuracy on the validation dataset: 0.267244\n",
            "epoch: 6, loss on the validation dataset: 1.776414\n",
            "\n",
            "Starting epoch 8/30, LR = [0.0001]\n",
            "epoch: 7, [iter: 1 / all 26], train_class_loss: 0.496146, train_domain_loss: 0.021616, test_domain_loss: 0.053831\n",
            "epoch: 7, [iter: 2 / all 26], train_class_loss: 0.555675, train_domain_loss: 0.015893, test_domain_loss: 0.064960\n",
            "epoch: 7, [iter: 3 / all 26], train_class_loss: 0.514278, train_domain_loss: 0.029048, test_domain_loss: 0.062069\n",
            "epoch: 7, [iter: 4 / all 26], train_class_loss: 0.519446, train_domain_loss: 0.047475, test_domain_loss: 0.058602\n",
            "epoch: 7, [iter: 5 / all 26], train_class_loss: 0.576839, train_domain_loss: 0.024937, test_domain_loss: 0.059057\n",
            "epoch: 7, [iter: 6 / all 26], train_class_loss: 0.484173, train_domain_loss: 0.016883, test_domain_loss: 0.050622\n",
            "epoch: 7, [iter: 7 / all 26], train_class_loss: 0.624569, train_domain_loss: 0.028174, test_domain_loss: 0.058801\n",
            "epoch: 7, [iter: 8 / all 26], train_class_loss: 0.485981, train_domain_loss: 0.050142, test_domain_loss: 0.058328\n",
            "epoch: 7, [iter: 9 / all 26], train_class_loss: 0.452655, train_domain_loss: 0.036067, test_domain_loss: 0.065064\n",
            "epoch: 7, [iter: 10 / all 26], train_class_loss: 0.542366, train_domain_loss: 0.029140, test_domain_loss: 0.058852\n",
            "epoch: 7, [iter: 11 / all 26], train_class_loss: 0.488770, train_domain_loss: 0.016383, test_domain_loss: 0.056345\n",
            "epoch: 7, [iter: 12 / all 26], train_class_loss: 0.514082, train_domain_loss: 0.031461, test_domain_loss: 0.048361\n",
            "epoch: 7, [iter: 13 / all 26], train_class_loss: 0.619318, train_domain_loss: 0.028384, test_domain_loss: 0.059846\n",
            "epoch: 7, [iter: 14 / all 26], train_class_loss: 0.512584, train_domain_loss: 0.020975, test_domain_loss: 0.068223\n",
            "epoch: 7, [iter: 15 / all 26], train_class_loss: 0.548229, train_domain_loss: 0.023079, test_domain_loss: 0.053573\n",
            "epoch: 7, [iter: 16 / all 26], train_class_loss: 0.557936, train_domain_loss: 0.019461, test_domain_loss: 0.061209\n",
            "epoch: 7, [iter: 17 / all 26], train_class_loss: 0.608829, train_domain_loss: 0.022084, test_domain_loss: 0.058292\n",
            "epoch: 7, [iter: 18 / all 26], train_class_loss: 0.461654, train_domain_loss: 0.020869, test_domain_loss: 0.060283\n",
            "epoch: 7, [iter: 19 / all 26], train_class_loss: 0.494125, train_domain_loss: 0.019036, test_domain_loss: 0.063721\n",
            "epoch: 7, [iter: 20 / all 26], train_class_loss: 0.524457, train_domain_loss: 0.010652, test_domain_loss: 0.047497\n",
            "epoch: 7, [iter: 21 / all 26], train_class_loss: 0.488001, train_domain_loss: 0.017914, test_domain_loss: 0.060469\n",
            "epoch: 7, [iter: 22 / all 26], train_class_loss: 0.425485, train_domain_loss: 0.012284, test_domain_loss: 0.060470\n",
            "epoch: 7, [iter: 23 / all 26], train_class_loss: 0.421603, train_domain_loss: 0.041492, test_domain_loss: 0.069941\n",
            "epoch: 7, [iter: 24 / all 26], train_class_loss: 0.531553, train_domain_loss: 0.019427, test_domain_loss: 0.059870\n",
            "epoch: 7, [iter: 25 / all 26], train_class_loss: 0.439865, train_domain_loss: 0.029201, test_domain_loss: 0.054723\n",
            "epoch: 7, [iter: 26 / all 26], train_class_loss: 0.441785, train_domain_loss: 0.015074, test_domain_loss: 0.064430\n",
            "Train class loss: 0.5127078695939138\n",
            "Train domain loss: 0.02489052200689912\n",
            "Val domain loss: 0.059132289714538135\n",
            "Total loss: 0.596730681315351\n",
            "Training Accuracy: 0.9332932692307693\n",
            "Training Loss: 0.37677406347714937\n",
            "epoch: 7, accuracy on the training dataset: 0.933293\n",
            "epoch: 7, loss on the training dataset: 0.376774\n",
            "Validation Accuracy: 0.28353270552303383\n",
            "Validation Loss: 1.7684042813608623\n",
            "epoch: 7, accuracy on the validation dataset: 0.283533\n",
            "epoch: 7, loss on the validation dataset: 1.768404\n",
            "\n",
            "Starting epoch 9/30, LR = [0.0001]\n",
            "epoch: 8, [iter: 1 / all 26], train_class_loss: 0.493253, train_domain_loss: 0.012769, test_domain_loss: 0.056469\n",
            "epoch: 8, [iter: 2 / all 26], train_class_loss: 0.407261, train_domain_loss: 0.034593, test_domain_loss: 0.059622\n",
            "epoch: 8, [iter: 3 / all 26], train_class_loss: 0.514448, train_domain_loss: 0.033874, test_domain_loss: 0.060882\n",
            "epoch: 8, [iter: 4 / all 26], train_class_loss: 0.473659, train_domain_loss: 0.012977, test_domain_loss: 0.063727\n",
            "epoch: 8, [iter: 5 / all 26], train_class_loss: 0.407446, train_domain_loss: 0.029384, test_domain_loss: 0.060503\n",
            "epoch: 8, [iter: 6 / all 26], train_class_loss: 0.520459, train_domain_loss: 0.017686, test_domain_loss: 0.051150\n",
            "epoch: 8, [iter: 7 / all 26], train_class_loss: 0.488343, train_domain_loss: 0.059637, test_domain_loss: 0.060731\n",
            "epoch: 8, [iter: 8 / all 26], train_class_loss: 0.453438, train_domain_loss: 0.027773, test_domain_loss: 0.060684\n",
            "epoch: 8, [iter: 9 / all 26], train_class_loss: 0.553868, train_domain_loss: 0.021798, test_domain_loss: 0.073021\n",
            "epoch: 8, [iter: 10 / all 26], train_class_loss: 0.439173, train_domain_loss: 0.014347, test_domain_loss: 0.066757\n",
            "epoch: 8, [iter: 11 / all 26], train_class_loss: 0.380026, train_domain_loss: 0.021871, test_domain_loss: 0.065844\n",
            "epoch: 8, [iter: 12 / all 26], train_class_loss: 0.350540, train_domain_loss: 0.007139, test_domain_loss: 0.056013\n",
            "epoch: 8, [iter: 13 / all 26], train_class_loss: 0.430181, train_domain_loss: 0.010792, test_domain_loss: 0.052501\n",
            "epoch: 8, [iter: 14 / all 26], train_class_loss: 0.483142, train_domain_loss: 0.021752, test_domain_loss: 0.063090\n",
            "epoch: 8, [iter: 15 / all 26], train_class_loss: 0.416473, train_domain_loss: 0.011552, test_domain_loss: 0.063720\n",
            "epoch: 8, [iter: 16 / all 26], train_class_loss: 0.386461, train_domain_loss: 0.013823, test_domain_loss: 0.061680\n",
            "epoch: 8, [iter: 17 / all 26], train_class_loss: 0.482775, train_domain_loss: 0.030698, test_domain_loss: 0.057562\n",
            "epoch: 8, [iter: 18 / all 26], train_class_loss: 0.424091, train_domain_loss: 0.043309, test_domain_loss: 0.056638\n",
            "epoch: 8, [iter: 19 / all 26], train_class_loss: 0.606652, train_domain_loss: 0.034832, test_domain_loss: 0.069262\n",
            "epoch: 8, [iter: 20 / all 26], train_class_loss: 0.598721, train_domain_loss: 0.044276, test_domain_loss: 0.062392\n",
            "epoch: 8, [iter: 21 / all 26], train_class_loss: 0.424452, train_domain_loss: 0.013189, test_domain_loss: 0.063791\n",
            "epoch: 8, [iter: 22 / all 26], train_class_loss: 0.374804, train_domain_loss: 0.022077, test_domain_loss: 0.069294\n",
            "epoch: 8, [iter: 23 / all 26], train_class_loss: 0.510986, train_domain_loss: 0.030783, test_domain_loss: 0.074469\n",
            "epoch: 8, [iter: 24 / all 26], train_class_loss: 0.373498, train_domain_loss: 0.022820, test_domain_loss: 0.058585\n",
            "epoch: 8, [iter: 25 / all 26], train_class_loss: 0.380317, train_domain_loss: 0.027332, test_domain_loss: 0.059010\n",
            "epoch: 8, [iter: 26 / all 26], train_class_loss: 0.483345, train_domain_loss: 0.058433, test_domain_loss: 0.060650\n",
            "Train class loss: 0.45606973423407626\n",
            "Train domain loss: 0.026135267080882423\n",
            "Val domain loss: 0.061848024622752115\n",
            "Total loss: 0.5440530259377108\n",
            "Training Accuracy: 0.9302884615384616\n",
            "Training Loss: 0.32843223214149475\n",
            "epoch: 8, accuracy on the training dataset: 0.930288\n",
            "epoch: 8, loss on the training dataset: 0.328432\n",
            "Validation Accuracy: 0.2542631712904047\n",
            "Validation Loss: 1.7723300677275469\n",
            "epoch: 8, accuracy on the validation dataset: 0.254263\n",
            "epoch: 8, loss on the validation dataset: 1.772330\n",
            "\n",
            "Starting epoch 10/30, LR = [0.0001]\n",
            "epoch: 9, [iter: 1 / all 26], train_class_loss: 0.495609, train_domain_loss: 0.018441, test_domain_loss: 0.060475\n",
            "epoch: 9, [iter: 2 / all 26], train_class_loss: 0.286294, train_domain_loss: 0.012659, test_domain_loss: 0.068822\n",
            "epoch: 9, [iter: 3 / all 26], train_class_loss: 0.394991, train_domain_loss: 0.010070, test_domain_loss: 0.067747\n",
            "epoch: 9, [iter: 4 / all 26], train_class_loss: 0.424146, train_domain_loss: 0.031613, test_domain_loss: 0.070964\n",
            "epoch: 9, [iter: 5 / all 26], train_class_loss: 0.480216, train_domain_loss: 0.022819, test_domain_loss: 0.060207\n",
            "epoch: 9, [iter: 6 / all 26], train_class_loss: 0.360099, train_domain_loss: 0.044695, test_domain_loss: 0.062948\n",
            "epoch: 9, [iter: 7 / all 26], train_class_loss: 0.396507, train_domain_loss: 0.007076, test_domain_loss: 0.065083\n",
            "epoch: 9, [iter: 8 / all 26], train_class_loss: 0.404702, train_domain_loss: 0.025562, test_domain_loss: 0.068332\n",
            "epoch: 9, [iter: 9 / all 26], train_class_loss: 0.400735, train_domain_loss: 0.029561, test_domain_loss: 0.072461\n",
            "epoch: 9, [iter: 10 / all 26], train_class_loss: 0.319760, train_domain_loss: 0.044246, test_domain_loss: 0.081054\n",
            "epoch: 9, [iter: 11 / all 26], train_class_loss: 0.342063, train_domain_loss: 0.005103, test_domain_loss: 0.077468\n",
            "epoch: 9, [iter: 12 / all 26], train_class_loss: 0.391088, train_domain_loss: 0.028080, test_domain_loss: 0.065484\n",
            "epoch: 9, [iter: 13 / all 26], train_class_loss: 0.320802, train_domain_loss: 0.018046, test_domain_loss: 0.062090\n",
            "epoch: 9, [iter: 14 / all 26], train_class_loss: 0.358741, train_domain_loss: 0.017200, test_domain_loss: 0.079105\n",
            "epoch: 9, [iter: 15 / all 26], train_class_loss: 0.376707, train_domain_loss: 0.039549, test_domain_loss: 0.071072\n",
            "epoch: 9, [iter: 16 / all 26], train_class_loss: 0.410193, train_domain_loss: 0.014897, test_domain_loss: 0.069773\n",
            "epoch: 9, [iter: 17 / all 26], train_class_loss: 0.351322, train_domain_loss: 0.017010, test_domain_loss: 0.073910\n",
            "epoch: 9, [iter: 18 / all 26], train_class_loss: 0.481857, train_domain_loss: 0.040701, test_domain_loss: 0.078189\n",
            "epoch: 9, [iter: 19 / all 26], train_class_loss: 0.315624, train_domain_loss: 0.025589, test_domain_loss: 0.082668\n",
            "epoch: 9, [iter: 20 / all 26], train_class_loss: 0.417768, train_domain_loss: 0.022411, test_domain_loss: 0.068423\n",
            "epoch: 9, [iter: 21 / all 26], train_class_loss: 0.353561, train_domain_loss: 0.031669, test_domain_loss: 0.080962\n",
            "epoch: 9, [iter: 22 / all 26], train_class_loss: 0.342882, train_domain_loss: 0.022471, test_domain_loss: 0.086745\n",
            "epoch: 9, [iter: 23 / all 26], train_class_loss: 0.403051, train_domain_loss: 0.021801, test_domain_loss: 0.089742\n",
            "epoch: 9, [iter: 24 / all 26], train_class_loss: 0.470099, train_domain_loss: 0.015208, test_domain_loss: 0.074404\n",
            "epoch: 9, [iter: 25 / all 26], train_class_loss: 0.472384, train_domain_loss: 0.018154, test_domain_loss: 0.069542\n",
            "epoch: 9, [iter: 26 / all 26], train_class_loss: 0.362486, train_domain_loss: 0.011497, test_domain_loss: 0.080592\n",
            "Train class loss: 0.38975708186626434\n",
            "Train domain loss: 0.022927888955634374\n",
            "Val domain loss: 0.07262557257826512\n",
            "Total loss: 0.48531054340016383\n",
            "Training Accuracy: 0.9308894230769231\n",
            "Training Loss: 0.2947707353876187\n",
            "epoch: 9, accuracy on the training dataset: 0.930889\n",
            "epoch: 9, loss on the training dataset: 0.294771\n",
            "Validation Accuracy: 0.27029778569610585\n",
            "Validation Loss: 1.7799085773988423\n",
            "epoch: 9, accuracy on the validation dataset: 0.270298\n",
            "epoch: 9, loss on the validation dataset: 1.779909\n",
            "\n",
            "Starting epoch 11/30, LR = [0.0001]\n",
            "epoch: 10, [iter: 1 / all 26], train_class_loss: 0.316625, train_domain_loss: 0.024354, test_domain_loss: 0.062165\n",
            "epoch: 10, [iter: 2 / all 26], train_class_loss: 0.295165, train_domain_loss: 0.006730, test_domain_loss: 0.077839\n",
            "epoch: 10, [iter: 3 / all 26], train_class_loss: 0.289552, train_domain_loss: 0.021489, test_domain_loss: 0.077910\n",
            "epoch: 10, [iter: 4 / all 26], train_class_loss: 0.265638, train_domain_loss: 0.034435, test_domain_loss: 0.079232\n",
            "epoch: 10, [iter: 5 / all 26], train_class_loss: 0.478523, train_domain_loss: 0.023428, test_domain_loss: 0.074932\n",
            "epoch: 10, [iter: 6 / all 26], train_class_loss: 0.307062, train_domain_loss: 0.027779, test_domain_loss: 0.074342\n",
            "epoch: 10, [iter: 7 / all 26], train_class_loss: 0.446091, train_domain_loss: 0.009836, test_domain_loss: 0.083621\n",
            "epoch: 10, [iter: 8 / all 26], train_class_loss: 0.321705, train_domain_loss: 0.027844, test_domain_loss: 0.092169\n",
            "epoch: 10, [iter: 9 / all 26], train_class_loss: 0.326509, train_domain_loss: 0.018392, test_domain_loss: 0.093827\n",
            "epoch: 10, [iter: 10 / all 26], train_class_loss: 0.296501, train_domain_loss: 0.012083, test_domain_loss: 0.083395\n",
            "epoch: 10, [iter: 11 / all 26], train_class_loss: 0.416782, train_domain_loss: 0.018174, test_domain_loss: 0.089894\n",
            "epoch: 10, [iter: 12 / all 26], train_class_loss: 0.328901, train_domain_loss: 0.021423, test_domain_loss: 0.074445\n",
            "epoch: 10, [iter: 13 / all 26], train_class_loss: 0.334863, train_domain_loss: 0.017869, test_domain_loss: 0.070750\n",
            "epoch: 10, [iter: 14 / all 26], train_class_loss: 0.431363, train_domain_loss: 0.019717, test_domain_loss: 0.079730\n",
            "epoch: 10, [iter: 15 / all 26], train_class_loss: 0.558207, train_domain_loss: 0.020718, test_domain_loss: 0.089649\n",
            "epoch: 10, [iter: 16 / all 26], train_class_loss: 0.332588, train_domain_loss: 0.020873, test_domain_loss: 0.085351\n",
            "epoch: 10, [iter: 17 / all 26], train_class_loss: 0.415817, train_domain_loss: 0.034635, test_domain_loss: 0.093415\n",
            "epoch: 10, [iter: 18 / all 26], train_class_loss: 0.392699, train_domain_loss: 0.052729, test_domain_loss: 0.089552\n",
            "epoch: 10, [iter: 19 / all 26], train_class_loss: 0.431834, train_domain_loss: 0.024981, test_domain_loss: 0.096859\n",
            "epoch: 10, [iter: 20 / all 26], train_class_loss: 0.333334, train_domain_loss: 0.009505, test_domain_loss: 0.091112\n",
            "epoch: 10, [iter: 21 / all 26], train_class_loss: 0.455718, train_domain_loss: 0.019042, test_domain_loss: 0.096689\n",
            "epoch: 10, [iter: 22 / all 26], train_class_loss: 0.319771, train_domain_loss: 0.026599, test_domain_loss: 0.097102\n",
            "epoch: 10, [iter: 23 / all 26], train_class_loss: 0.390714, train_domain_loss: 0.043217, test_domain_loss: 0.112014\n",
            "epoch: 10, [iter: 24 / all 26], train_class_loss: 0.283173, train_domain_loss: 0.030811, test_domain_loss: 0.088256\n",
            "epoch: 10, [iter: 25 / all 26], train_class_loss: 0.421898, train_domain_loss: 0.033111, test_domain_loss: 0.062314\n",
            "epoch: 10, [iter: 26 / all 26], train_class_loss: 0.401744, train_domain_loss: 0.027590, test_domain_loss: 0.093991\n",
            "Train class loss: 0.36895290934122527\n",
            "Train domain loss: 0.02412941690104512\n",
            "Val domain loss: 0.08502144595751396\n",
            "Total loss: 0.47810377219978434\n",
            "Training Accuracy: 0.9362980769230769\n",
            "Training Loss: 0.2613622901531366\n",
            "epoch: 10, accuracy on the training dataset: 0.936298\n",
            "epoch: 10, loss on the training dataset: 0.261362\n",
            "Validation Accuracy: 0.25655383049121916\n",
            "Validation Loss: 1.8010348131044367\n",
            "epoch: 10, accuracy on the validation dataset: 0.256554\n",
            "epoch: 10, loss on the validation dataset: 1.801035\n",
            "\n",
            "Starting epoch 12/30, LR = [0.0001]\n",
            "epoch: 11, [iter: 1 / all 26], train_class_loss: 0.197813, train_domain_loss: 0.032826, test_domain_loss: 0.064392\n",
            "epoch: 11, [iter: 2 / all 26], train_class_loss: 0.328011, train_domain_loss: 0.081567, test_domain_loss: 0.092909\n",
            "epoch: 11, [iter: 3 / all 26], train_class_loss: 0.367563, train_domain_loss: 0.010079, test_domain_loss: 0.102918\n",
            "epoch: 11, [iter: 4 / all 26], train_class_loss: 0.341286, train_domain_loss: 0.036219, test_domain_loss: 0.107603\n",
            "epoch: 11, [iter: 5 / all 26], train_class_loss: 0.337946, train_domain_loss: 0.025486, test_domain_loss: 0.101805\n",
            "epoch: 11, [iter: 6 / all 26], train_class_loss: 0.289362, train_domain_loss: 0.010766, test_domain_loss: 0.094573\n",
            "epoch: 11, [iter: 7 / all 26], train_class_loss: 0.274811, train_domain_loss: 0.015886, test_domain_loss: 0.105439\n",
            "epoch: 11, [iter: 8 / all 26], train_class_loss: 0.281714, train_domain_loss: 0.019933, test_domain_loss: 0.108667\n",
            "epoch: 11, [iter: 9 / all 26], train_class_loss: 0.370367, train_domain_loss: 0.022710, test_domain_loss: 0.106839\n",
            "epoch: 11, [iter: 10 / all 26], train_class_loss: 0.415712, train_domain_loss: 0.018928, test_domain_loss: 0.105675\n",
            "epoch: 11, [iter: 11 / all 26], train_class_loss: 0.256604, train_domain_loss: 0.012393, test_domain_loss: 0.111211\n",
            "epoch: 11, [iter: 12 / all 26], train_class_loss: 0.365021, train_domain_loss: 0.044436, test_domain_loss: 0.111879\n",
            "epoch: 11, [iter: 13 / all 26], train_class_loss: 0.306973, train_domain_loss: 0.009289, test_domain_loss: 0.085506\n",
            "epoch: 11, [iter: 14 / all 26], train_class_loss: 0.338236, train_domain_loss: 0.013439, test_domain_loss: 0.111380\n",
            "epoch: 11, [iter: 15 / all 26], train_class_loss: 0.416946, train_domain_loss: 0.022325, test_domain_loss: 0.120315\n",
            "epoch: 11, [iter: 16 / all 26], train_class_loss: 0.244890, train_domain_loss: 0.022462, test_domain_loss: 0.117293\n",
            "epoch: 11, [iter: 17 / all 26], train_class_loss: 0.332162, train_domain_loss: 0.019440, test_domain_loss: 0.119170\n",
            "epoch: 11, [iter: 18 / all 26], train_class_loss: 0.326594, train_domain_loss: 0.022308, test_domain_loss: 0.131839\n",
            "epoch: 11, [iter: 19 / all 26], train_class_loss: 0.311046, train_domain_loss: 0.025754, test_domain_loss: 0.132376\n",
            "epoch: 11, [iter: 20 / all 26], train_class_loss: 0.412539, train_domain_loss: 0.013087, test_domain_loss: 0.124015\n",
            "epoch: 11, [iter: 21 / all 26], train_class_loss: 0.239686, train_domain_loss: 0.090408, test_domain_loss: 0.134489\n",
            "epoch: 11, [iter: 22 / all 26], train_class_loss: 0.510291, train_domain_loss: 0.019657, test_domain_loss: 0.134149\n",
            "epoch: 11, [iter: 23 / all 26], train_class_loss: 0.313754, train_domain_loss: 0.015726, test_domain_loss: 0.147373\n",
            "epoch: 11, [iter: 24 / all 26], train_class_loss: 0.247221, train_domain_loss: 0.013966, test_domain_loss: 0.106505\n",
            "epoch: 11, [iter: 25 / all 26], train_class_loss: 0.269545, train_domain_loss: 0.035161, test_domain_loss: 0.075096\n",
            "epoch: 11, [iter: 26 / all 26], train_class_loss: 0.307507, train_domain_loss: 0.017746, test_domain_loss: 0.117827\n",
            "Train class loss: 0.3232153012202336\n",
            "Train domain loss: 0.02584599581762002\n",
            "Val domain loss: 0.11043248153649844\n",
            "Total loss: 0.4594937785743521\n",
            "Training Accuracy: 0.9399038461538461\n",
            "Training Loss: 0.24643163325694892\n",
            "epoch: 11, accuracy on the training dataset: 0.939904\n",
            "epoch: 11, loss on the training dataset: 0.246432\n",
            "Validation Accuracy: 0.2430643929753118\n",
            "Validation Loss: 1.8264484863664758\n",
            "epoch: 11, accuracy on the validation dataset: 0.243064\n",
            "epoch: 11, loss on the validation dataset: 1.826448\n",
            "\n",
            "Starting epoch 13/30, LR = [0.0001]\n",
            "epoch: 12, [iter: 1 / all 26], train_class_loss: 0.289096, train_domain_loss: 0.008760, test_domain_loss: 0.101940\n",
            "epoch: 12, [iter: 2 / all 26], train_class_loss: 0.413890, train_domain_loss: 0.031803, test_domain_loss: 0.115774\n",
            "epoch: 12, [iter: 3 / all 26], train_class_loss: 0.316234, train_domain_loss: 0.046514, test_domain_loss: 0.128097\n",
            "epoch: 12, [iter: 4 / all 26], train_class_loss: 0.335326, train_domain_loss: 0.019190, test_domain_loss: 0.137645\n",
            "epoch: 12, [iter: 5 / all 26], train_class_loss: 0.261681, train_domain_loss: 0.006585, test_domain_loss: 0.133183\n",
            "epoch: 12, [iter: 6 / all 26], train_class_loss: 0.233589, train_domain_loss: 0.013429, test_domain_loss: 0.117635\n",
            "epoch: 12, [iter: 7 / all 26], train_class_loss: 0.202748, train_domain_loss: 0.010905, test_domain_loss: 0.142183\n",
            "epoch: 12, [iter: 8 / all 26], train_class_loss: 0.321972, train_domain_loss: 0.026362, test_domain_loss: 0.133712\n",
            "epoch: 12, [iter: 9 / all 26], train_class_loss: 0.281453, train_domain_loss: 0.012064, test_domain_loss: 0.141307\n",
            "epoch: 12, [iter: 10 / all 26], train_class_loss: 0.212582, train_domain_loss: 0.025042, test_domain_loss: 0.136673\n",
            "epoch: 12, [iter: 11 / all 26], train_class_loss: 0.274341, train_domain_loss: 0.034569, test_domain_loss: 0.152343\n",
            "epoch: 12, [iter: 12 / all 26], train_class_loss: 0.351640, train_domain_loss: 0.040272, test_domain_loss: 0.142943\n",
            "epoch: 12, [iter: 13 / all 26], train_class_loss: 0.259718, train_domain_loss: 0.016206, test_domain_loss: 0.104763\n",
            "epoch: 12, [iter: 14 / all 26], train_class_loss: 0.246037, train_domain_loss: 0.060641, test_domain_loss: 0.142014\n",
            "epoch: 12, [iter: 15 / all 26], train_class_loss: 0.331842, train_domain_loss: 0.061045, test_domain_loss: 0.156454\n",
            "epoch: 12, [iter: 16 / all 26], train_class_loss: 0.401904, train_domain_loss: 0.049206, test_domain_loss: 0.142302\n",
            "epoch: 12, [iter: 17 / all 26], train_class_loss: 0.341771, train_domain_loss: 0.082990, test_domain_loss: 0.149526\n",
            "epoch: 12, [iter: 18 / all 26], train_class_loss: 0.307054, train_domain_loss: 0.038791, test_domain_loss: 0.159161\n",
            "epoch: 12, [iter: 19 / all 26], train_class_loss: 0.394735, train_domain_loss: 0.016061, test_domain_loss: 0.173624\n",
            "epoch: 12, [iter: 20 / all 26], train_class_loss: 0.268632, train_domain_loss: 0.009918, test_domain_loss: 0.171250\n",
            "epoch: 12, [iter: 21 / all 26], train_class_loss: 0.266296, train_domain_loss: 0.010779, test_domain_loss: 0.178859\n",
            "epoch: 12, [iter: 22 / all 26], train_class_loss: 0.364376, train_domain_loss: 0.038401, test_domain_loss: 0.181796\n",
            "epoch: 12, [iter: 23 / all 26], train_class_loss: 0.201617, train_domain_loss: 0.034060, test_domain_loss: 0.178595\n",
            "epoch: 12, [iter: 24 / all 26], train_class_loss: 0.305912, train_domain_loss: 0.020878, test_domain_loss: 0.146949\n",
            "epoch: 12, [iter: 25 / all 26], train_class_loss: 0.445612, train_domain_loss: 0.041165, test_domain_loss: 0.101740\n",
            "epoch: 12, [iter: 26 / all 26], train_class_loss: 0.232062, train_domain_loss: 0.029699, test_domain_loss: 0.156482\n",
            "Train class loss: 0.30238931798017943\n",
            "Train domain loss: 0.030205207017178718\n",
            "Val domain loss: 0.14334426447749138\n",
            "Total loss: 0.4759387894748495\n",
            "Training Accuracy: 0.9435096153846154\n",
            "Training Loss: 0.23327749795638597\n",
            "epoch: 12, accuracy on the training dataset: 0.943510\n",
            "epoch: 12, loss on the training dataset: 0.233277\n",
            "Validation Accuracy: 0.23110206159328073\n",
            "Validation Loss: 1.8529529117026928\n",
            "epoch: 12, accuracy on the validation dataset: 0.231102\n",
            "epoch: 12, loss on the validation dataset: 1.852953\n",
            "\n",
            "Starting epoch 14/30, LR = [0.0001]\n",
            "epoch: 13, [iter: 1 / all 26], train_class_loss: 0.291129, train_domain_loss: 0.012519, test_domain_loss: 0.119589\n",
            "epoch: 13, [iter: 2 / all 26], train_class_loss: 0.273381, train_domain_loss: 0.028662, test_domain_loss: 0.160093\n",
            "epoch: 13, [iter: 3 / all 26], train_class_loss: 0.286851, train_domain_loss: 0.018892, test_domain_loss: 0.176046\n",
            "epoch: 13, [iter: 4 / all 26], train_class_loss: 0.319788, train_domain_loss: 0.047044, test_domain_loss: 0.174731\n",
            "epoch: 13, [iter: 5 / all 26], train_class_loss: 0.263326, train_domain_loss: 0.041857, test_domain_loss: 0.175368\n",
            "epoch: 13, [iter: 6 / all 26], train_class_loss: 0.221736, train_domain_loss: 0.035749, test_domain_loss: 0.160459\n",
            "epoch: 13, [iter: 7 / all 26], train_class_loss: 0.245660, train_domain_loss: 0.046168, test_domain_loss: 0.178361\n",
            "epoch: 13, [iter: 8 / all 26], train_class_loss: 0.276755, train_domain_loss: 0.026033, test_domain_loss: 0.171921\n",
            "epoch: 13, [iter: 9 / all 26], train_class_loss: 0.338819, train_domain_loss: 0.014617, test_domain_loss: 0.169211\n",
            "epoch: 13, [iter: 10 / all 26], train_class_loss: 0.221147, train_domain_loss: 0.006547, test_domain_loss: 0.178033\n",
            "epoch: 13, [iter: 11 / all 26], train_class_loss: 0.259555, train_domain_loss: 0.045960, test_domain_loss: 0.204674\n",
            "epoch: 13, [iter: 12 / all 26], train_class_loss: 0.356934, train_domain_loss: 0.020090, test_domain_loss: 0.191671\n",
            "epoch: 13, [iter: 13 / all 26], train_class_loss: 0.295593, train_domain_loss: 0.020613, test_domain_loss: 0.146712\n",
            "epoch: 13, [iter: 14 / all 26], train_class_loss: 0.273156, train_domain_loss: 0.011681, test_domain_loss: 0.182838\n",
            "epoch: 13, [iter: 15 / all 26], train_class_loss: 0.168380, train_domain_loss: 0.004277, test_domain_loss: 0.200887\n",
            "epoch: 13, [iter: 16 / all 26], train_class_loss: 0.228194, train_domain_loss: 0.015637, test_domain_loss: 0.192565\n",
            "epoch: 13, [iter: 17 / all 26], train_class_loss: 0.242337, train_domain_loss: 0.003337, test_domain_loss: 0.197936\n",
            "epoch: 13, [iter: 18 / all 26], train_class_loss: 0.239806, train_domain_loss: 0.024900, test_domain_loss: 0.188195\n",
            "epoch: 13, [iter: 19 / all 26], train_class_loss: 0.242490, train_domain_loss: 0.029665, test_domain_loss: 0.224321\n",
            "epoch: 13, [iter: 20 / all 26], train_class_loss: 0.254344, train_domain_loss: 0.039221, test_domain_loss: 0.197550\n",
            "epoch: 13, [iter: 21 / all 26], train_class_loss: 0.287310, train_domain_loss: 0.014434, test_domain_loss: 0.229106\n",
            "epoch: 13, [iter: 22 / all 26], train_class_loss: 0.244918, train_domain_loss: 0.064769, test_domain_loss: 0.211612\n",
            "epoch: 13, [iter: 23 / all 26], train_class_loss: 0.339761, train_domain_loss: 0.125937, test_domain_loss: 0.212292\n",
            "epoch: 13, [iter: 24 / all 26], train_class_loss: 0.233279, train_domain_loss: 0.020250, test_domain_loss: 0.176757\n",
            "epoch: 13, [iter: 25 / all 26], train_class_loss: 0.448661, train_domain_loss: 0.024099, test_domain_loss: 0.131939\n",
            "epoch: 13, [iter: 26 / all 26], train_class_loss: 0.332269, train_domain_loss: 0.009575, test_domain_loss: 0.208548\n",
            "Train class loss: 0.2763685022409146\n",
            "Train domain loss: 0.028943605774727005\n",
            "Val domain loss: 0.18313131825282022\n",
            "Total loss: 0.4884434262684618\n",
            "Training Accuracy: 0.9393028846153846\n",
            "Training Loss: 0.21896730076808196\n",
            "epoch: 13, accuracy on the training dataset: 0.939303\n",
            "epoch: 13, loss on the training dataset: 0.218967\n",
            "Validation Accuracy: 0.22575719012471368\n",
            "Validation Loss: 1.873031087672932\n",
            "epoch: 13, accuracy on the validation dataset: 0.225757\n",
            "epoch: 13, loss on the validation dataset: 1.873031\n",
            "\n",
            "Starting epoch 15/30, LR = [0.0001]\n",
            "epoch: 14, [iter: 1 / all 26], train_class_loss: 0.231928, train_domain_loss: 0.007340, test_domain_loss: 0.155568\n",
            "epoch: 14, [iter: 2 / all 26], train_class_loss: 0.240226, train_domain_loss: 0.019432, test_domain_loss: 0.200979\n",
            "epoch: 14, [iter: 3 / all 26], train_class_loss: 0.187325, train_domain_loss: 0.020029, test_domain_loss: 0.222791\n",
            "epoch: 14, [iter: 4 / all 26], train_class_loss: 0.210726, train_domain_loss: 0.077804, test_domain_loss: 0.216865\n",
            "epoch: 14, [iter: 5 / all 26], train_class_loss: 0.366435, train_domain_loss: 0.046011, test_domain_loss: 0.208702\n",
            "epoch: 14, [iter: 6 / all 26], train_class_loss: 0.320597, train_domain_loss: 0.014811, test_domain_loss: 0.204920\n",
            "epoch: 14, [iter: 7 / all 26], train_class_loss: 0.330500, train_domain_loss: 0.021307, test_domain_loss: 0.225958\n",
            "epoch: 14, [iter: 8 / all 26], train_class_loss: 0.293464, train_domain_loss: 0.034285, test_domain_loss: 0.215615\n",
            "epoch: 14, [iter: 9 / all 26], train_class_loss: 0.263715, train_domain_loss: 0.022264, test_domain_loss: 0.210535\n",
            "epoch: 14, [iter: 10 / all 26], train_class_loss: 0.374918, train_domain_loss: 0.032766, test_domain_loss: 0.212916\n",
            "epoch: 14, [iter: 11 / all 26], train_class_loss: 0.275432, train_domain_loss: 0.008957, test_domain_loss: 0.244696\n",
            "epoch: 14, [iter: 12 / all 26], train_class_loss: 0.207445, train_domain_loss: 0.036204, test_domain_loss: 0.246918\n",
            "epoch: 14, [iter: 13 / all 26], train_class_loss: 0.273047, train_domain_loss: 0.052003, test_domain_loss: 0.179640\n",
            "epoch: 14, [iter: 14 / all 26], train_class_loss: 0.216566, train_domain_loss: 0.013709, test_domain_loss: 0.225542\n",
            "epoch: 14, [iter: 15 / all 26], train_class_loss: 0.177741, train_domain_loss: 0.007591, test_domain_loss: 0.261157\n",
            "epoch: 14, [iter: 16 / all 26], train_class_loss: 0.315740, train_domain_loss: 0.055976, test_domain_loss: 0.228375\n",
            "epoch: 14, [iter: 17 / all 26], train_class_loss: 0.408354, train_domain_loss: 0.033601, test_domain_loss: 0.245889\n",
            "epoch: 14, [iter: 18 / all 26], train_class_loss: 0.226525, train_domain_loss: 0.016994, test_domain_loss: 0.239986\n",
            "epoch: 14, [iter: 19 / all 26], train_class_loss: 0.133116, train_domain_loss: 0.050122, test_domain_loss: 0.261851\n",
            "epoch: 14, [iter: 20 / all 26], train_class_loss: 0.212855, train_domain_loss: 0.014333, test_domain_loss: 0.243239\n",
            "epoch: 14, [iter: 21 / all 26], train_class_loss: 0.297486, train_domain_loss: 0.038202, test_domain_loss: 0.278385\n",
            "epoch: 14, [iter: 22 / all 26], train_class_loss: 0.288884, train_domain_loss: 0.007918, test_domain_loss: 0.275922\n",
            "epoch: 14, [iter: 23 / all 26], train_class_loss: 0.287676, train_domain_loss: 0.015250, test_domain_loss: 0.274706\n",
            "epoch: 14, [iter: 24 / all 26], train_class_loss: 0.165466, train_domain_loss: 0.039533, test_domain_loss: 0.241638\n",
            "epoch: 14, [iter: 25 / all 26], train_class_loss: 0.287186, train_domain_loss: 0.036710, test_domain_loss: 0.190816\n",
            "epoch: 14, [iter: 26 / all 26], train_class_loss: 0.371635, train_domain_loss: 0.012430, test_domain_loss: 0.254603\n",
            "Train class loss: 0.2678840584479846\n",
            "Train domain loss: 0.028291617394783176\n",
            "Val domain loss: 0.22954650395191634\n",
            "Total loss: 0.525722179794684\n",
            "Training Accuracy: 0.9381009615384616\n",
            "Training Loss: 0.21829353645443916\n",
            "epoch: 14, accuracy on the training dataset: 0.938101\n",
            "epoch: 14, loss on the training dataset: 0.218294\n",
            "Validation Accuracy: 0.2234665309238992\n",
            "Validation Loss: 1.8896559434678604\n",
            "epoch: 14, accuracy on the validation dataset: 0.223467\n",
            "epoch: 14, loss on the validation dataset: 1.889656\n",
            "\n",
            "Starting epoch 16/30, LR = [0.0001]\n",
            "epoch: 15, [iter: 1 / all 26], train_class_loss: 0.192189, train_domain_loss: 0.053908, test_domain_loss: 0.208761\n",
            "epoch: 15, [iter: 2 / all 26], train_class_loss: 0.277988, train_domain_loss: 0.009581, test_domain_loss: 0.253328\n",
            "epoch: 15, [iter: 3 / all 26], train_class_loss: 0.218030, train_domain_loss: 0.001758, test_domain_loss: 0.258534\n",
            "epoch: 15, [iter: 4 / all 26], train_class_loss: 0.296216, train_domain_loss: 0.045721, test_domain_loss: 0.277653\n",
            "epoch: 15, [iter: 5 / all 26], train_class_loss: 0.249177, train_domain_loss: 0.009044, test_domain_loss: 0.267145\n",
            "epoch: 15, [iter: 6 / all 26], train_class_loss: 0.283572, train_domain_loss: 0.024030, test_domain_loss: 0.258866\n",
            "epoch: 15, [iter: 7 / all 26], train_class_loss: 0.264899, train_domain_loss: 0.021817, test_domain_loss: 0.287466\n",
            "epoch: 15, [iter: 8 / all 26], train_class_loss: 0.281543, train_domain_loss: 0.029189, test_domain_loss: 0.273825\n",
            "epoch: 15, [iter: 9 / all 26], train_class_loss: 0.191007, train_domain_loss: 0.026079, test_domain_loss: 0.268546\n",
            "epoch: 15, [iter: 10 / all 26], train_class_loss: 0.225404, train_domain_loss: 0.018682, test_domain_loss: 0.282853\n",
            "epoch: 15, [iter: 11 / all 26], train_class_loss: 0.233690, train_domain_loss: 0.014601, test_domain_loss: 0.303834\n",
            "epoch: 15, [iter: 12 / all 26], train_class_loss: 0.265298, train_domain_loss: 0.037388, test_domain_loss: 0.306096\n",
            "epoch: 15, [iter: 13 / all 26], train_class_loss: 0.173334, train_domain_loss: 0.062734, test_domain_loss: 0.239221\n",
            "epoch: 15, [iter: 14 / all 26], train_class_loss: 0.273990, train_domain_loss: 0.040655, test_domain_loss: 0.294339\n",
            "epoch: 15, [iter: 15 / all 26], train_class_loss: 0.342984, train_domain_loss: 0.042745, test_domain_loss: 0.329940\n",
            "epoch: 15, [iter: 16 / all 26], train_class_loss: 0.219089, train_domain_loss: 0.020976, test_domain_loss: 0.287190\n",
            "epoch: 15, [iter: 17 / all 26], train_class_loss: 0.269745, train_domain_loss: 0.037700, test_domain_loss: 0.310390\n",
            "epoch: 15, [iter: 18 / all 26], train_class_loss: 0.381768, train_domain_loss: 0.047142, test_domain_loss: 0.301995\n",
            "epoch: 15, [iter: 19 / all 26], train_class_loss: 0.181344, train_domain_loss: 0.007204, test_domain_loss: 0.320134\n",
            "epoch: 15, [iter: 20 / all 26], train_class_loss: 0.238521, train_domain_loss: 0.049760, test_domain_loss: 0.311375\n",
            "epoch: 15, [iter: 21 / all 26], train_class_loss: 0.219077, train_domain_loss: 0.059139, test_domain_loss: 0.340131\n",
            "epoch: 15, [iter: 22 / all 26], train_class_loss: 0.372594, train_domain_loss: 0.035602, test_domain_loss: 0.336467\n",
            "epoch: 15, [iter: 23 / all 26], train_class_loss: 0.195844, train_domain_loss: 0.005896, test_domain_loss: 0.319974\n",
            "epoch: 15, [iter: 24 / all 26], train_class_loss: 0.209948, train_domain_loss: 0.013540, test_domain_loss: 0.293825\n",
            "epoch: 15, [iter: 25 / all 26], train_class_loss: 0.267445, train_domain_loss: 0.019952, test_domain_loss: 0.252021\n",
            "epoch: 15, [iter: 26 / all 26], train_class_loss: 0.263672, train_domain_loss: 0.004807, test_domain_loss: 0.309179\n",
            "Train class loss: 0.2533987451058168\n",
            "Train domain loss: 0.02844803723005148\n",
            "Val domain loss: 0.2881956667854236\n",
            "Total loss: 0.5700424491212919\n",
            "Training Accuracy: 0.9567307692307693\n",
            "Training Loss: 0.18471462652087212\n",
            "epoch: 15, accuracy on the training dataset: 0.956731\n",
            "epoch: 15, loss on the training dataset: 0.184715\n",
            "Validation Accuracy: 0.20972257571901248\n",
            "Validation Loss: 1.9018416506062636\n",
            "epoch: 15, accuracy on the validation dataset: 0.209723\n",
            "epoch: 15, loss on the validation dataset: 1.901842\n",
            "\n",
            "Starting epoch 17/30, LR = [0.0001]\n",
            "epoch: 16, [iter: 1 / all 26], train_class_loss: 0.247340, train_domain_loss: 0.027058, test_domain_loss: 0.271074\n",
            "epoch: 16, [iter: 2 / all 26], train_class_loss: 0.156950, train_domain_loss: 0.013687, test_domain_loss: 0.320389\n",
            "epoch: 16, [iter: 3 / all 26], train_class_loss: 0.237734, train_domain_loss: 0.029587, test_domain_loss: 0.336758\n",
            "epoch: 16, [iter: 4 / all 26], train_class_loss: 0.123028, train_domain_loss: 0.003380, test_domain_loss: 0.349477\n",
            "epoch: 16, [iter: 5 / all 26], train_class_loss: 0.270939, train_domain_loss: 0.026338, test_domain_loss: 0.345406\n",
            "epoch: 16, [iter: 6 / all 26], train_class_loss: 0.479827, train_domain_loss: 0.019497, test_domain_loss: 0.325104\n",
            "epoch: 16, [iter: 7 / all 26], train_class_loss: 0.203196, train_domain_loss: 0.038997, test_domain_loss: 0.368677\n",
            "epoch: 16, [iter: 8 / all 26], train_class_loss: 0.270051, train_domain_loss: 0.018588, test_domain_loss: 0.330887\n",
            "epoch: 16, [iter: 9 / all 26], train_class_loss: 0.307980, train_domain_loss: 0.026731, test_domain_loss: 0.340275\n",
            "epoch: 16, [iter: 10 / all 26], train_class_loss: 0.242602, train_domain_loss: 0.016703, test_domain_loss: 0.344783\n",
            "epoch: 16, [iter: 11 / all 26], train_class_loss: 0.283770, train_domain_loss: 0.048687, test_domain_loss: 0.374123\n",
            "epoch: 16, [iter: 12 / all 26], train_class_loss: 0.272677, train_domain_loss: 0.027144, test_domain_loss: 0.373651\n",
            "epoch: 16, [iter: 13 / all 26], train_class_loss: 0.337863, train_domain_loss: 0.034891, test_domain_loss: 0.309415\n",
            "epoch: 16, [iter: 14 / all 26], train_class_loss: 0.176229, train_domain_loss: 0.032454, test_domain_loss: 0.368179\n",
            "epoch: 16, [iter: 15 / all 26], train_class_loss: 0.191120, train_domain_loss: 0.007761, test_domain_loss: 0.407761\n",
            "epoch: 16, [iter: 16 / all 26], train_class_loss: 0.252127, train_domain_loss: 0.014807, test_domain_loss: 0.370334\n",
            "epoch: 16, [iter: 17 / all 26], train_class_loss: 0.367047, train_domain_loss: 0.006446, test_domain_loss: 0.383801\n",
            "epoch: 16, [iter: 18 / all 26], train_class_loss: 0.201229, train_domain_loss: 0.005734, test_domain_loss: 0.374793\n",
            "epoch: 16, [iter: 19 / all 26], train_class_loss: 0.306624, train_domain_loss: 0.010896, test_domain_loss: 0.403051\n",
            "epoch: 16, [iter: 20 / all 26], train_class_loss: 0.179301, train_domain_loss: 0.025840, test_domain_loss: 0.378392\n",
            "epoch: 16, [iter: 21 / all 26], train_class_loss: 0.235752, train_domain_loss: 0.030317, test_domain_loss: 0.419406\n",
            "epoch: 16, [iter: 22 / all 26], train_class_loss: 0.257531, train_domain_loss: 0.051435, test_domain_loss: 0.410012\n",
            "epoch: 16, [iter: 23 / all 26], train_class_loss: 0.321087, train_domain_loss: 0.052469, test_domain_loss: 0.404372\n",
            "epoch: 16, [iter: 24 / all 26], train_class_loss: 0.145683, train_domain_loss: 0.056075, test_domain_loss: 0.391005\n",
            "epoch: 16, [iter: 25 / all 26], train_class_loss: 0.388642, train_domain_loss: 0.015169, test_domain_loss: 0.330385\n",
            "epoch: 16, [iter: 26 / all 26], train_class_loss: 0.170729, train_domain_loss: 0.016940, test_domain_loss: 0.396290\n",
            "Train class loss: 0.25488690831340277\n",
            "Train domain loss: 0.025293507971442662\n",
            "Val domain loss: 0.3626077473163605\n",
            "Total loss: 0.6427881636012059\n",
            "Training Accuracy: 0.9519230769230769\n",
            "Training Loss: 0.18838889438372391\n",
            "epoch: 16, accuracy on the training dataset: 0.951923\n",
            "epoch: 16, loss on the training dataset: 0.188389\n",
            "Validation Accuracy: 0.18757953677780606\n",
            "Validation Loss: 1.9128948460707806\n",
            "epoch: 16, accuracy on the validation dataset: 0.187580\n",
            "epoch: 16, loss on the validation dataset: 1.912895\n",
            "\n",
            "Starting epoch 18/30, LR = [0.0001]\n",
            "epoch: 17, [iter: 1 / all 26], train_class_loss: 0.195512, train_domain_loss: 0.006998, test_domain_loss: 0.356679\n",
            "epoch: 17, [iter: 2 / all 26], train_class_loss: 0.208290, train_domain_loss: 0.004074, test_domain_loss: 0.397369\n",
            "epoch: 17, [iter: 3 / all 26], train_class_loss: 0.194140, train_domain_loss: 0.006728, test_domain_loss: 0.412572\n",
            "epoch: 17, [iter: 4 / all 26], train_class_loss: 0.267589, train_domain_loss: 0.094678, test_domain_loss: 0.422464\n",
            "epoch: 17, [iter: 5 / all 26], train_class_loss: 0.200776, train_domain_loss: 0.040434, test_domain_loss: 0.424952\n",
            "epoch: 17, [iter: 6 / all 26], train_class_loss: 0.283486, train_domain_loss: 0.030273, test_domain_loss: 0.399057\n",
            "epoch: 17, [iter: 7 / all 26], train_class_loss: 0.215846, train_domain_loss: 0.048451, test_domain_loss: 0.436206\n",
            "epoch: 17, [iter: 8 / all 26], train_class_loss: 0.276813, train_domain_loss: 0.018944, test_domain_loss: 0.409528\n",
            "epoch: 17, [iter: 9 / all 26], train_class_loss: 0.159308, train_domain_loss: 0.026621, test_domain_loss: 0.427749\n",
            "epoch: 17, [iter: 10 / all 26], train_class_loss: 0.286711, train_domain_loss: 0.046000, test_domain_loss: 0.432794\n",
            "epoch: 17, [iter: 11 / all 26], train_class_loss: 0.200366, train_domain_loss: 0.080810, test_domain_loss: 0.445234\n",
            "epoch: 17, [iter: 12 / all 26], train_class_loss: 0.165392, train_domain_loss: 0.030371, test_domain_loss: 0.439683\n",
            "epoch: 17, [iter: 13 / all 26], train_class_loss: 0.168166, train_domain_loss: 0.007751, test_domain_loss: 0.396033\n",
            "epoch: 17, [iter: 14 / all 26], train_class_loss: 0.338278, train_domain_loss: 0.016663, test_domain_loss: 0.440186\n",
            "epoch: 17, [iter: 15 / all 26], train_class_loss: 0.270521, train_domain_loss: 0.025832, test_domain_loss: 0.467687\n",
            "epoch: 17, [iter: 16 / all 26], train_class_loss: 0.328648, train_domain_loss: 0.011747, test_domain_loss: 0.444699\n",
            "epoch: 17, [iter: 17 / all 26], train_class_loss: 0.302043, train_domain_loss: 0.021545, test_domain_loss: 0.472879\n",
            "epoch: 17, [iter: 18 / all 26], train_class_loss: 0.182566, train_domain_loss: 0.026358, test_domain_loss: 0.454269\n",
            "epoch: 17, [iter: 19 / all 26], train_class_loss: 0.334315, train_domain_loss: 0.033223, test_domain_loss: 0.480440\n",
            "epoch: 17, [iter: 20 / all 26], train_class_loss: 0.224068, train_domain_loss: 0.008232, test_domain_loss: 0.460734\n",
            "epoch: 17, [iter: 21 / all 26], train_class_loss: 0.243891, train_domain_loss: 0.026890, test_domain_loss: 0.484257\n",
            "epoch: 17, [iter: 22 / all 26], train_class_loss: 0.213391, train_domain_loss: 0.012158, test_domain_loss: 0.492705\n",
            "epoch: 17, [iter: 23 / all 26], train_class_loss: 0.292446, train_domain_loss: 0.027316, test_domain_loss: 0.468674\n",
            "epoch: 17, [iter: 24 / all 26], train_class_loss: 0.210280, train_domain_loss: 0.008990, test_domain_loss: 0.452690\n",
            "epoch: 17, [iter: 25 / all 26], train_class_loss: 0.303535, train_domain_loss: 0.011043, test_domain_loss: 0.415069\n",
            "epoch: 17, [iter: 26 / all 26], train_class_loss: 0.246642, train_domain_loss: 0.021136, test_domain_loss: 0.459138\n",
            "Train class loss: 0.24280844514186567\n",
            "Train domain loss: 0.02666410253956341\n",
            "Val domain loss: 0.4382209823681758\n",
            "Total loss: 0.7076935300496049\n",
            "Training Accuracy: 0.9525240384615384\n",
            "Training Loss: 0.18789666489912912\n",
            "epoch: 17, accuracy on the training dataset: 0.952524\n",
            "epoch: 17, loss on the training dataset: 0.187897\n",
            "Validation Accuracy: 0.1707813693051667\n",
            "Validation Loss: 1.920006520786368\n",
            "epoch: 17, accuracy on the validation dataset: 0.170781\n",
            "epoch: 17, loss on the validation dataset: 1.920007\n",
            "\n",
            "Starting epoch 19/30, LR = [0.0001]\n",
            "epoch: 18, [iter: 1 / all 26], train_class_loss: 0.210059, train_domain_loss: 0.006387, test_domain_loss: 0.425917\n",
            "epoch: 18, [iter: 2 / all 26], train_class_loss: 0.181594, train_domain_loss: 0.016300, test_domain_loss: 0.472707\n",
            "epoch: 18, [iter: 3 / all 26], train_class_loss: 0.251492, train_domain_loss: 0.017131, test_domain_loss: 0.471317\n",
            "epoch: 18, [iter: 4 / all 26], train_class_loss: 0.264197, train_domain_loss: 0.029458, test_domain_loss: 0.483232\n",
            "epoch: 18, [iter: 5 / all 26], train_class_loss: 0.222125, train_domain_loss: 0.026232, test_domain_loss: 0.487109\n",
            "epoch: 18, [iter: 6 / all 26], train_class_loss: 0.177060, train_domain_loss: 0.045192, test_domain_loss: 0.469742\n",
            "epoch: 18, [iter: 7 / all 26], train_class_loss: 0.173734, train_domain_loss: 0.038324, test_domain_loss: 0.484326\n",
            "epoch: 18, [iter: 8 / all 26], train_class_loss: 0.244619, train_domain_loss: 0.034972, test_domain_loss: 0.469510\n",
            "epoch: 18, [iter: 9 / all 26], train_class_loss: 0.227955, train_domain_loss: 0.026479, test_domain_loss: 0.476063\n",
            "epoch: 18, [iter: 10 / all 26], train_class_loss: 0.227788, train_domain_loss: 0.002192, test_domain_loss: 0.475115\n",
            "epoch: 18, [iter: 11 / all 26], train_class_loss: 0.210335, train_domain_loss: 0.036140, test_domain_loss: 0.488499\n",
            "epoch: 18, [iter: 12 / all 26], train_class_loss: 0.145991, train_domain_loss: 0.013850, test_domain_loss: 0.499261\n",
            "epoch: 18, [iter: 13 / all 26], train_class_loss: 0.237885, train_domain_loss: 0.002343, test_domain_loss: 0.453800\n",
            "epoch: 18, [iter: 14 / all 26], train_class_loss: 0.271182, train_domain_loss: 0.056708, test_domain_loss: 0.488630\n",
            "epoch: 18, [iter: 15 / all 26], train_class_loss: 0.196864, train_domain_loss: 0.008879, test_domain_loss: 0.521392\n",
            "epoch: 18, [iter: 16 / all 26], train_class_loss: 0.320148, train_domain_loss: 0.055768, test_domain_loss: 0.493401\n",
            "epoch: 18, [iter: 17 / all 26], train_class_loss: 0.181752, train_domain_loss: 0.006658, test_domain_loss: 0.511704\n",
            "epoch: 18, [iter: 18 / all 26], train_class_loss: 0.259221, train_domain_loss: 0.022646, test_domain_loss: 0.501862\n",
            "epoch: 18, [iter: 19 / all 26], train_class_loss: 0.278775, train_domain_loss: 0.008848, test_domain_loss: 0.515155\n",
            "epoch: 18, [iter: 20 / all 26], train_class_loss: 0.284473, train_domain_loss: 0.071142, test_domain_loss: 0.502826\n",
            "epoch: 18, [iter: 21 / all 26], train_class_loss: 0.220029, train_domain_loss: 0.042259, test_domain_loss: 0.515448\n",
            "epoch: 18, [iter: 22 / all 26], train_class_loss: 0.270532, train_domain_loss: 0.058105, test_domain_loss: 0.511199\n",
            "epoch: 18, [iter: 23 / all 26], train_class_loss: 0.219145, train_domain_loss: 0.021005, test_domain_loss: 0.513815\n",
            "epoch: 18, [iter: 24 / all 26], train_class_loss: 0.340560, train_domain_loss: 0.025309, test_domain_loss: 0.499221\n",
            "epoch: 18, [iter: 25 / all 26], train_class_loss: 0.156484, train_domain_loss: 0.006261, test_domain_loss: 0.459590\n",
            "epoch: 18, [iter: 26 / all 26], train_class_loss: 0.244858, train_domain_loss: 0.050377, test_domain_loss: 0.489765\n",
            "Train class loss: 0.23149444850591513\n",
            "Train domain loss: 0.028037170234781045\n",
            "Val domain loss: 0.48771564547832197\n",
            "Total loss: 0.7472472642190181\n",
            "Training Accuracy: 0.9621394230769231\n",
            "Training Loss: 0.16263061905136475\n",
            "epoch: 18, accuracy on the training dataset: 0.962139\n",
            "epoch: 18, loss on the training dataset: 0.162631\n",
            "Validation Accuracy: 0.1885976075337236\n",
            "Validation Loss: 1.924697970397359\n",
            "epoch: 18, accuracy on the validation dataset: 0.188598\n",
            "epoch: 18, loss on the validation dataset: 1.924698\n",
            "\n",
            "Starting epoch 20/30, LR = [0.0001]\n",
            "epoch: 19, [iter: 1 / all 26], train_class_loss: 0.206266, train_domain_loss: 0.023952, test_domain_loss: 0.463252\n",
            "epoch: 19, [iter: 2 / all 26], train_class_loss: 0.152312, train_domain_loss: 0.014414, test_domain_loss: 0.508760\n",
            "epoch: 19, [iter: 3 / all 26], train_class_loss: 0.204187, train_domain_loss: 0.010662, test_domain_loss: 0.512522\n",
            "epoch: 19, [iter: 4 / all 26], train_class_loss: 0.303297, train_domain_loss: 0.025183, test_domain_loss: 0.524342\n",
            "epoch: 19, [iter: 5 / all 26], train_class_loss: 0.250364, train_domain_loss: 0.020039, test_domain_loss: 0.520066\n",
            "epoch: 19, [iter: 6 / all 26], train_class_loss: 0.206985, train_domain_loss: 0.024766, test_domain_loss: 0.501583\n",
            "epoch: 19, [iter: 7 / all 26], train_class_loss: 0.227458, train_domain_loss: 0.028457, test_domain_loss: 0.527153\n",
            "epoch: 19, [iter: 8 / all 26], train_class_loss: 0.220778, train_domain_loss: 0.017952, test_domain_loss: 0.515011\n",
            "epoch: 19, [iter: 9 / all 26], train_class_loss: 0.298578, train_domain_loss: 0.004634, test_domain_loss: 0.510757\n",
            "epoch: 19, [iter: 10 / all 26], train_class_loss: 0.218022, train_domain_loss: 0.012883, test_domain_loss: 0.514334\n",
            "epoch: 19, [iter: 11 / all 26], train_class_loss: 0.342117, train_domain_loss: 0.016958, test_domain_loss: 0.516332\n",
            "epoch: 19, [iter: 12 / all 26], train_class_loss: 0.242942, train_domain_loss: 0.004281, test_domain_loss: 0.526290\n",
            "epoch: 19, [iter: 13 / all 26], train_class_loss: 0.191115, train_domain_loss: 0.001641, test_domain_loss: 0.498074\n",
            "epoch: 19, [iter: 14 / all 26], train_class_loss: 0.225463, train_domain_loss: 0.020586, test_domain_loss: 0.529695\n",
            "epoch: 19, [iter: 15 / all 26], train_class_loss: 0.261758, train_domain_loss: 0.036199, test_domain_loss: 0.539340\n",
            "epoch: 19, [iter: 16 / all 26], train_class_loss: 0.231622, train_domain_loss: 0.033954, test_domain_loss: 0.524364\n",
            "epoch: 19, [iter: 17 / all 26], train_class_loss: 0.196294, train_domain_loss: 0.028239, test_domain_loss: 0.535308\n",
            "epoch: 19, [iter: 18 / all 26], train_class_loss: 0.138546, train_domain_loss: 0.005001, test_domain_loss: 0.532159\n",
            "epoch: 19, [iter: 19 / all 26], train_class_loss: 0.201642, train_domain_loss: 0.008463, test_domain_loss: 0.539456\n",
            "epoch: 19, [iter: 20 / all 26], train_class_loss: 0.123467, train_domain_loss: 0.009497, test_domain_loss: 0.528711\n",
            "epoch: 19, [iter: 21 / all 26], train_class_loss: 0.145033, train_domain_loss: 0.012371, test_domain_loss: 0.539222\n",
            "epoch: 19, [iter: 22 / all 26], train_class_loss: 0.207787, train_domain_loss: 0.012220, test_domain_loss: 0.543431\n",
            "epoch: 19, [iter: 23 / all 26], train_class_loss: 0.320050, train_domain_loss: 0.034790, test_domain_loss: 0.538899\n",
            "epoch: 19, [iter: 24 / all 26], train_class_loss: 0.271909, train_domain_loss: 0.022798, test_domain_loss: 0.522638\n",
            "epoch: 19, [iter: 25 / all 26], train_class_loss: 0.214911, train_domain_loss: 0.024171, test_domain_loss: 0.499905\n",
            "epoch: 19, [iter: 26 / all 26], train_class_loss: 0.333054, train_domain_loss: 0.018091, test_domain_loss: 0.524565\n",
            "Train class loss: 0.2283061224107559\n",
            "Train domain loss: 0.018161668478009794\n",
            "Val domain loss: 0.5206218740114799\n",
            "Total loss: 0.7670896649002455\n",
            "Training Accuracy: 0.9507211538461539\n",
            "Training Loss: 0.17473664564582017\n",
            "epoch: 19, accuracy on the training dataset: 0.950721\n",
            "epoch: 19, loss on the training dataset: 0.174737\n",
            "Validation Accuracy: 0.20692288114023924\n",
            "Validation Loss: 1.9273501447698\n",
            "epoch: 19, accuracy on the validation dataset: 0.206923\n",
            "epoch: 19, loss on the validation dataset: 1.927350\n",
            "\n",
            "Starting epoch 21/30, LR = [1.0000000000000002e-06]\n",
            "epoch: 20, [iter: 1 / all 26], train_class_loss: 0.247489, train_domain_loss: 0.025534, test_domain_loss: 0.503719\n",
            "epoch: 20, [iter: 2 / all 26], train_class_loss: 0.348427, train_domain_loss: 0.053891, test_domain_loss: 0.527381\n",
            "epoch: 20, [iter: 3 / all 26], train_class_loss: 0.207050, train_domain_loss: 0.022026, test_domain_loss: 0.535134\n",
            "epoch: 20, [iter: 4 / all 26], train_class_loss: 0.228877, train_domain_loss: 0.036762, test_domain_loss: 0.533493\n",
            "epoch: 20, [iter: 5 / all 26], train_class_loss: 0.293422, train_domain_loss: 0.002537, test_domain_loss: 0.532053\n",
            "epoch: 20, [iter: 6 / all 26], train_class_loss: 0.143298, train_domain_loss: 0.015630, test_domain_loss: 0.527692\n",
            "epoch: 20, [iter: 7 / all 26], train_class_loss: 0.255428, train_domain_loss: 0.003440, test_domain_loss: 0.538192\n",
            "epoch: 20, [iter: 8 / all 26], train_class_loss: 0.184408, train_domain_loss: 0.019395, test_domain_loss: 0.531027\n",
            "epoch: 20, [iter: 9 / all 26], train_class_loss: 0.175218, train_domain_loss: 0.008871, test_domain_loss: 0.532945\n",
            "epoch: 20, [iter: 10 / all 26], train_class_loss: 0.213709, train_domain_loss: 0.013209, test_domain_loss: 0.529899\n",
            "epoch: 20, [iter: 11 / all 26], train_class_loss: 0.275775, train_domain_loss: 0.030979, test_domain_loss: 0.533940\n",
            "epoch: 20, [iter: 12 / all 26], train_class_loss: 0.186015, train_domain_loss: 0.019986, test_domain_loss: 0.538985\n",
            "epoch: 20, [iter: 13 / all 26], train_class_loss: 0.245480, train_domain_loss: 0.031884, test_domain_loss: 0.514078\n",
            "epoch: 20, [iter: 14 / all 26], train_class_loss: 0.214367, train_domain_loss: 0.013160, test_domain_loss: 0.534910\n",
            "epoch: 20, [iter: 15 / all 26], train_class_loss: 0.137436, train_domain_loss: 0.015315, test_domain_loss: 0.547607\n",
            "epoch: 20, [iter: 16 / all 26], train_class_loss: 0.233340, train_domain_loss: 0.014097, test_domain_loss: 0.536062\n",
            "epoch: 20, [iter: 17 / all 26], train_class_loss: 0.178490, train_domain_loss: 0.003223, test_domain_loss: 0.538855\n",
            "epoch: 20, [iter: 18 / all 26], train_class_loss: 0.200564, train_domain_loss: 0.015807, test_domain_loss: 0.535992\n",
            "epoch: 20, [iter: 19 / all 26], train_class_loss: 0.168191, train_domain_loss: 0.005711, test_domain_loss: 0.545452\n",
            "epoch: 20, [iter: 20 / all 26], train_class_loss: 0.171460, train_domain_loss: 0.017026, test_domain_loss: 0.529711\n",
            "epoch: 20, [iter: 21 / all 26], train_class_loss: 0.263541, train_domain_loss: 0.026786, test_domain_loss: 0.545862\n",
            "epoch: 20, [iter: 22 / all 26], train_class_loss: 0.233245, train_domain_loss: 0.015986, test_domain_loss: 0.545518\n",
            "epoch: 20, [iter: 23 / all 26], train_class_loss: 0.202624, train_domain_loss: 0.004541, test_domain_loss: 0.536596\n",
            "epoch: 20, [iter: 24 / all 26], train_class_loss: 0.216081, train_domain_loss: 0.019499, test_domain_loss: 0.526396\n",
            "epoch: 20, [iter: 25 / all 26], train_class_loss: 0.229933, train_domain_loss: 0.022229, test_domain_loss: 0.499375\n",
            "epoch: 20, [iter: 26 / all 26], train_class_loss: 0.182050, train_domain_loss: 0.018234, test_domain_loss: 0.530793\n",
            "Train class loss: 0.21676615682932046\n",
            "Train domain loss: 0.018298378464980766\n",
            "Val domain loss: 0.5319871891003388\n",
            "Total loss: 0.7670517243946401\n",
            "Training Accuracy: 0.9603365384615384\n",
            "Training Loss: 0.16298189701942298\n",
            "epoch: 20, accuracy on the training dataset: 0.960337\n",
            "epoch: 20, loss on the training dataset: 0.162982\n",
            "Validation Accuracy: 0.22168490710104352\n",
            "Validation Loss: 1.9275761833321992\n",
            "epoch: 20, accuracy on the validation dataset: 0.221685\n",
            "epoch: 20, loss on the validation dataset: 1.927576\n",
            "\n",
            "Starting epoch 22/30, LR = [1e-05]\n",
            "epoch: 21, [iter: 1 / all 26], train_class_loss: 0.205755, train_domain_loss: 0.028389, test_domain_loss: 0.505445\n",
            "epoch: 21, [iter: 2 / all 26], train_class_loss: 0.249653, train_domain_loss: 0.033634, test_domain_loss: 0.528400\n",
            "epoch: 21, [iter: 3 / all 26], train_class_loss: 0.194081, train_domain_loss: 0.009483, test_domain_loss: 0.531461\n",
            "epoch: 21, [iter: 4 / all 26], train_class_loss: 0.187735, train_domain_loss: 0.031843, test_domain_loss: 0.539880\n",
            "epoch: 21, [iter: 5 / all 26], train_class_loss: 0.269644, train_domain_loss: 0.027074, test_domain_loss: 0.536781\n",
            "epoch: 21, [iter: 6 / all 26], train_class_loss: 0.197119, train_domain_loss: 0.040839, test_domain_loss: 0.521773\n",
            "epoch: 21, [iter: 7 / all 26], train_class_loss: 0.197088, train_domain_loss: 0.015483, test_domain_loss: 0.539453\n",
            "epoch: 21, [iter: 8 / all 26], train_class_loss: 0.265517, train_domain_loss: 0.005077, test_domain_loss: 0.533882\n",
            "epoch: 21, [iter: 9 / all 26], train_class_loss: 0.221281, train_domain_loss: 0.010788, test_domain_loss: 0.532602\n",
            "epoch: 21, [iter: 10 / all 26], train_class_loss: 0.316369, train_domain_loss: 0.035399, test_domain_loss: 0.528152\n",
            "epoch: 21, [iter: 11 / all 26], train_class_loss: 0.229596, train_domain_loss: 0.012962, test_domain_loss: 0.537193\n",
            "epoch: 21, [iter: 12 / all 26], train_class_loss: 0.274901, train_domain_loss: 0.008915, test_domain_loss: 0.540614\n",
            "epoch: 21, [iter: 13 / all 26], train_class_loss: 0.129738, train_domain_loss: 0.003468, test_domain_loss: 0.512947\n",
            "epoch: 21, [iter: 14 / all 26], train_class_loss: 0.262845, train_domain_loss: 0.001671, test_domain_loss: 0.537203\n",
            "epoch: 21, [iter: 15 / all 26], train_class_loss: 0.170154, train_domain_loss: 0.002284, test_domain_loss: 0.552119\n",
            "epoch: 21, [iter: 16 / all 26], train_class_loss: 0.301281, train_domain_loss: 0.006433, test_domain_loss: 0.535251\n",
            "epoch: 21, [iter: 17 / all 26], train_class_loss: 0.104381, train_domain_loss: 0.011551, test_domain_loss: 0.544364\n",
            "epoch: 21, [iter: 18 / all 26], train_class_loss: 0.220615, train_domain_loss: 0.017654, test_domain_loss: 0.532536\n",
            "epoch: 21, [iter: 19 / all 26], train_class_loss: 0.173581, train_domain_loss: 0.003120, test_domain_loss: 0.544856\n",
            "epoch: 21, [iter: 20 / all 26], train_class_loss: 0.171054, train_domain_loss: 0.000987, test_domain_loss: 0.536945\n",
            "epoch: 21, [iter: 21 / all 26], train_class_loss: 0.309942, train_domain_loss: 0.004693, test_domain_loss: 0.546011\n",
            "epoch: 21, [iter: 22 / all 26], train_class_loss: 0.175953, train_domain_loss: 0.033374, test_domain_loss: 0.543884\n",
            "epoch: 21, [iter: 23 / all 26], train_class_loss: 0.178552, train_domain_loss: 0.035689, test_domain_loss: 0.538963\n",
            "epoch: 21, [iter: 24 / all 26], train_class_loss: 0.237497, train_domain_loss: 0.016581, test_domain_loss: 0.525658\n",
            "epoch: 21, [iter: 25 / all 26], train_class_loss: 0.201650, train_domain_loss: 0.004476, test_domain_loss: 0.502250\n",
            "epoch: 21, [iter: 26 / all 26], train_class_loss: 0.152860, train_domain_loss: 0.028648, test_domain_loss: 0.527253\n",
            "Train class loss: 0.21534008991259795\n",
            "Train domain loss: 0.01655827032831999\n",
            "Val domain loss: 0.532918237722837\n",
            "Total loss: 0.7648165979637549\n",
            "Training Accuracy: 0.9615384615384616\n",
            "Training Loss: 0.15704758236041436\n",
            "epoch: 21, accuracy on the training dataset: 0.961538\n",
            "epoch: 21, loss on the training dataset: 0.157048\n",
            "Validation Accuracy: 0.23593789768388904\n",
            "Validation Loss: 1.9277905757325877\n",
            "epoch: 21, accuracy on the validation dataset: 0.235938\n",
            "epoch: 21, loss on the validation dataset: 1.927791\n",
            "\n",
            "Starting epoch 23/30, LR = [1e-05]\n",
            "epoch: 22, [iter: 1 / all 26], train_class_loss: 0.335362, train_domain_loss: 0.037331, test_domain_loss: 0.506849\n",
            "epoch: 22, [iter: 2 / all 26], train_class_loss: 0.144180, train_domain_loss: 0.002924, test_domain_loss: 0.537194\n",
            "epoch: 22, [iter: 3 / all 26], train_class_loss: 0.156532, train_domain_loss: 0.000635, test_domain_loss: 0.535725\n",
            "epoch: 22, [iter: 4 / all 26], train_class_loss: 0.188201, train_domain_loss: 0.028572, test_domain_loss: 0.539095\n",
            "epoch: 22, [iter: 5 / all 26], train_class_loss: 0.216570, train_domain_loss: 0.028073, test_domain_loss: 0.537886\n",
            "epoch: 22, [iter: 6 / all 26], train_class_loss: 0.250925, train_domain_loss: 0.063408, test_domain_loss: 0.524011\n",
            "epoch: 22, [iter: 7 / all 26], train_class_loss: 0.213964, train_domain_loss: 0.001259, test_domain_loss: 0.541780\n",
            "epoch: 22, [iter: 8 / all 26], train_class_loss: 0.161349, train_domain_loss: 0.006005, test_domain_loss: 0.535882\n",
            "epoch: 22, [iter: 9 / all 26], train_class_loss: 0.197544, train_domain_loss: 0.004763, test_domain_loss: 0.536181\n",
            "epoch: 22, [iter: 10 / all 26], train_class_loss: 0.176943, train_domain_loss: 0.008127, test_domain_loss: 0.535071\n",
            "epoch: 22, [iter: 11 / all 26], train_class_loss: 0.277606, train_domain_loss: 0.009095, test_domain_loss: 0.538118\n",
            "epoch: 22, [iter: 12 / all 26], train_class_loss: 0.181917, train_domain_loss: 0.035342, test_domain_loss: 0.539916\n",
            "epoch: 22, [iter: 13 / all 26], train_class_loss: 0.153887, train_domain_loss: 0.044479, test_domain_loss: 0.510649\n",
            "epoch: 22, [iter: 14 / all 26], train_class_loss: 0.281802, train_domain_loss: 0.035102, test_domain_loss: 0.538413\n",
            "epoch: 22, [iter: 15 / all 26], train_class_loss: 0.157900, train_domain_loss: 0.027306, test_domain_loss: 0.548742\n",
            "epoch: 22, [iter: 16 / all 26], train_class_loss: 0.215660, train_domain_loss: 0.009904, test_domain_loss: 0.534640\n",
            "epoch: 22, [iter: 17 / all 26], train_class_loss: 0.189494, train_domain_loss: 0.012462, test_domain_loss: 0.541850\n",
            "epoch: 22, [iter: 18 / all 26], train_class_loss: 0.147437, train_domain_loss: 0.002930, test_domain_loss: 0.536977\n",
            "epoch: 22, [iter: 19 / all 26], train_class_loss: 0.181814, train_domain_loss: 0.015424, test_domain_loss: 0.549105\n",
            "epoch: 22, [iter: 20 / all 26], train_class_loss: 0.220835, train_domain_loss: 0.011938, test_domain_loss: 0.539140\n",
            "epoch: 22, [iter: 21 / all 26], train_class_loss: 0.226320, train_domain_loss: 0.004757, test_domain_loss: 0.539944\n",
            "epoch: 22, [iter: 22 / all 26], train_class_loss: 0.175407, train_domain_loss: 0.004917, test_domain_loss: 0.545228\n",
            "epoch: 22, [iter: 23 / all 26], train_class_loss: 0.237265, train_domain_loss: 0.005406, test_domain_loss: 0.540559\n",
            "epoch: 22, [iter: 24 / all 26], train_class_loss: 0.254884, train_domain_loss: 0.002326, test_domain_loss: 0.530529\n",
            "epoch: 22, [iter: 25 / all 26], train_class_loss: 0.225646, train_domain_loss: 0.014937, test_domain_loss: 0.501195\n",
            "epoch: 22, [iter: 26 / all 26], train_class_loss: 0.196248, train_domain_loss: 0.007250, test_domain_loss: 0.536488\n",
            "Train class loss: 0.20637265993998602\n",
            "Train domain loss: 0.016333585748305686\n",
            "Val domain loss: 0.5346602774583377\n",
            "Total loss: 0.7573665231466293\n",
            "Training Accuracy: 0.9615384615384616\n",
            "Training Loss: 0.15480268345429346\n",
            "epoch: 22, accuracy on the training dataset: 0.961538\n",
            "epoch: 22, loss on the training dataset: 0.154803\n",
            "Validation Accuracy: 0.24255535759735303\n",
            "Validation Loss: 1.9279873837766892\n",
            "epoch: 22, accuracy on the validation dataset: 0.242555\n",
            "epoch: 22, loss on the validation dataset: 1.927987\n",
            "\n",
            "Starting epoch 24/30, LR = [1e-05]\n",
            "epoch: 23, [iter: 1 / all 26], train_class_loss: 0.204957, train_domain_loss: 0.028401, test_domain_loss: 0.513354\n",
            "epoch: 23, [iter: 2 / all 26], train_class_loss: 0.222231, train_domain_loss: 0.003371, test_domain_loss: 0.531363\n",
            "epoch: 23, [iter: 3 / all 26], train_class_loss: 0.154456, train_domain_loss: 0.018826, test_domain_loss: 0.536027\n",
            "epoch: 23, [iter: 4 / all 26], train_class_loss: 0.151768, train_domain_loss: 0.017768, test_domain_loss: 0.535153\n",
            "epoch: 23, [iter: 5 / all 26], train_class_loss: 0.157356, train_domain_loss: 0.010188, test_domain_loss: 0.538705\n",
            "epoch: 23, [iter: 6 / all 26], train_class_loss: 0.266889, train_domain_loss: 0.051248, test_domain_loss: 0.526567\n",
            "epoch: 23, [iter: 7 / all 26], train_class_loss: 0.213740, train_domain_loss: 0.024081, test_domain_loss: 0.541758\n",
            "epoch: 23, [iter: 8 / all 26], train_class_loss: 0.187726, train_domain_loss: 0.053415, test_domain_loss: 0.531708\n",
            "epoch: 23, [iter: 9 / all 26], train_class_loss: 0.208927, train_domain_loss: 0.007359, test_domain_loss: 0.533451\n",
            "epoch: 23, [iter: 10 / all 26], train_class_loss: 0.233977, train_domain_loss: 0.015342, test_domain_loss: 0.535842\n",
            "epoch: 23, [iter: 11 / all 26], train_class_loss: 0.189637, train_domain_loss: 0.019694, test_domain_loss: 0.545349\n",
            "epoch: 23, [iter: 12 / all 26], train_class_loss: 0.307920, train_domain_loss: 0.034267, test_domain_loss: 0.543084\n",
            "epoch: 23, [iter: 13 / all 26], train_class_loss: 0.251636, train_domain_loss: 0.009196, test_domain_loss: 0.514989\n",
            "epoch: 23, [iter: 14 / all 26], train_class_loss: 0.202661, train_domain_loss: 0.001084, test_domain_loss: 0.539446\n",
            "epoch: 23, [iter: 15 / all 26], train_class_loss: 0.166783, train_domain_loss: 0.004008, test_domain_loss: 0.549922\n",
            "epoch: 23, [iter: 16 / all 26], train_class_loss: 0.190054, train_domain_loss: 0.015921, test_domain_loss: 0.538262\n",
            "epoch: 23, [iter: 17 / all 26], train_class_loss: 0.262432, train_domain_loss: 0.005120, test_domain_loss: 0.543710\n",
            "epoch: 23, [iter: 18 / all 26], train_class_loss: 0.149320, train_domain_loss: 0.004137, test_domain_loss: 0.539565\n",
            "epoch: 23, [iter: 19 / all 26], train_class_loss: 0.220730, train_domain_loss: 0.016759, test_domain_loss: 0.549015\n",
            "epoch: 23, [iter: 20 / all 26], train_class_loss: 0.232846, train_domain_loss: 0.008921, test_domain_loss: 0.538812\n",
            "epoch: 23, [iter: 21 / all 26], train_class_loss: 0.229228, train_domain_loss: 0.029460, test_domain_loss: 0.546836\n",
            "epoch: 23, [iter: 22 / all 26], train_class_loss: 0.171468, train_domain_loss: 0.015121, test_domain_loss: 0.540930\n",
            "epoch: 23, [iter: 23 / all 26], train_class_loss: 0.191083, train_domain_loss: 0.011121, test_domain_loss: 0.537727\n",
            "epoch: 23, [iter: 24 / all 26], train_class_loss: 0.148099, train_domain_loss: 0.000863, test_domain_loss: 0.536584\n",
            "epoch: 23, [iter: 25 / all 26], train_class_loss: 0.212943, train_domain_loss: 0.030593, test_domain_loss: 0.507615\n",
            "epoch: 23, [iter: 26 / all 26], train_class_loss: 0.197633, train_domain_loss: 0.007306, test_domain_loss: 0.530089\n",
            "Train class loss: 0.20486541894766\n",
            "Train domain loss: 0.017060303774017554\n",
            "Val domain loss: 0.5356100889352652\n",
            "Total loss: 0.7575358116569427\n",
            "Training Accuracy: 0.9627403846153846\n",
            "Training Loss: 0.15425237239553377\n",
            "epoch: 23, accuracy on the training dataset: 0.962740\n",
            "epoch: 23, loss on the training dataset: 0.154252\n",
            "Validation Accuracy: 0.24535505217612624\n",
            "Validation Loss: 1.9281535936269665\n",
            "epoch: 23, accuracy on the validation dataset: 0.245355\n",
            "epoch: 23, loss on the validation dataset: 1.928154\n",
            "\n",
            "Starting epoch 25/30, LR = [1e-05]\n",
            "epoch: 24, [iter: 1 / all 26], train_class_loss: 0.160965, train_domain_loss: 0.000786, test_domain_loss: 0.513402\n",
            "epoch: 24, [iter: 2 / all 26], train_class_loss: 0.188622, train_domain_loss: 0.006330, test_domain_loss: 0.534100\n",
            "epoch: 24, [iter: 3 / all 26], train_class_loss: 0.238775, train_domain_loss: 0.034529, test_domain_loss: 0.536056\n",
            "epoch: 24, [iter: 4 / all 26], train_class_loss: 0.180701, train_domain_loss: 0.014064, test_domain_loss: 0.536495\n",
            "epoch: 24, [iter: 5 / all 26], train_class_loss: 0.227915, train_domain_loss: 0.017861, test_domain_loss: 0.538165\n",
            "epoch: 24, [iter: 6 / all 26], train_class_loss: 0.267293, train_domain_loss: 0.016561, test_domain_loss: 0.524239\n",
            "epoch: 24, [iter: 7 / all 26], train_class_loss: 0.191510, train_domain_loss: 0.007295, test_domain_loss: 0.542823\n",
            "epoch: 24, [iter: 8 / all 26], train_class_loss: 0.125597, train_domain_loss: 0.012671, test_domain_loss: 0.535227\n",
            "epoch: 24, [iter: 9 / all 26], train_class_loss: 0.309651, train_domain_loss: 0.024240, test_domain_loss: 0.535189\n",
            "epoch: 24, [iter: 10 / all 26], train_class_loss: 0.188774, train_domain_loss: 0.030567, test_domain_loss: 0.535865\n",
            "epoch: 24, [iter: 11 / all 26], train_class_loss: 0.201624, train_domain_loss: 0.002008, test_domain_loss: 0.536493\n",
            "epoch: 24, [iter: 12 / all 26], train_class_loss: 0.238763, train_domain_loss: 0.002028, test_domain_loss: 0.540555\n",
            "epoch: 24, [iter: 13 / all 26], train_class_loss: 0.236107, train_domain_loss: 0.002645, test_domain_loss: 0.521865\n",
            "epoch: 24, [iter: 14 / all 26], train_class_loss: 0.286935, train_domain_loss: 0.001236, test_domain_loss: 0.543461\n",
            "epoch: 24, [iter: 15 / all 26], train_class_loss: 0.162085, train_domain_loss: 0.051919, test_domain_loss: 0.546884\n",
            "epoch: 24, [iter: 16 / all 26], train_class_loss: 0.272374, train_domain_loss: 0.056928, test_domain_loss: 0.539947\n",
            "epoch: 24, [iter: 17 / all 26], train_class_loss: 0.272609, train_domain_loss: 0.009527, test_domain_loss: 0.541978\n",
            "epoch: 24, [iter: 18 / all 26], train_class_loss: 0.191712, train_domain_loss: 0.019831, test_domain_loss: 0.540139\n",
            "epoch: 24, [iter: 19 / all 26], train_class_loss: 0.240209, train_domain_loss: 0.003919, test_domain_loss: 0.545230\n",
            "epoch: 24, [iter: 20 / all 26], train_class_loss: 0.163938, train_domain_loss: 0.011240, test_domain_loss: 0.541286\n",
            "epoch: 24, [iter: 21 / all 26], train_class_loss: 0.186564, train_domain_loss: 0.010522, test_domain_loss: 0.546963\n",
            "epoch: 24, [iter: 22 / all 26], train_class_loss: 0.221943, train_domain_loss: 0.017704, test_domain_loss: 0.545105\n",
            "epoch: 24, [iter: 23 / all 26], train_class_loss: 0.126597, train_domain_loss: 0.001593, test_domain_loss: 0.544376\n",
            "epoch: 24, [iter: 24 / all 26], train_class_loss: 0.302822, train_domain_loss: 0.028226, test_domain_loss: 0.529164\n",
            "epoch: 24, [iter: 25 / all 26], train_class_loss: 0.200334, train_domain_loss: 0.016440, test_domain_loss: 0.512314\n",
            "epoch: 24, [iter: 26 / all 26], train_class_loss: 0.111286, train_domain_loss: 0.017136, test_domain_loss: 0.536797\n",
            "Train class loss: 0.21137330738397744\n",
            "Train domain loss: 0.01606953226459714\n",
            "Val domain loss: 0.5363122270657465\n",
            "Total loss: 0.7637550667143211\n",
            "Training Accuracy: 0.9627403846153846\n",
            "Training Loss: 0.1528849799472552\n",
            "epoch: 24, accuracy on the training dataset: 0.962740\n",
            "epoch: 24, loss on the training dataset: 0.152885\n",
            "Validation Accuracy: 0.2506999236446933\n",
            "Validation Loss: 1.928261104962637\n",
            "epoch: 24, accuracy on the validation dataset: 0.250700\n",
            "epoch: 24, loss on the validation dataset: 1.928261\n",
            "\n",
            "Starting epoch 26/30, LR = [1e-05]\n",
            "epoch: 25, [iter: 1 / all 26], train_class_loss: 0.173890, train_domain_loss: 0.017929, test_domain_loss: 0.513989\n",
            "epoch: 25, [iter: 2 / all 26], train_class_loss: 0.105032, train_domain_loss: 0.009222, test_domain_loss: 0.532296\n",
            "epoch: 25, [iter: 3 / all 26], train_class_loss: 0.188030, train_domain_loss: 0.001650, test_domain_loss: 0.539051\n",
            "epoch: 25, [iter: 4 / all 26], train_class_loss: 0.142599, train_domain_loss: 0.017890, test_domain_loss: 0.541948\n",
            "epoch: 25, [iter: 5 / all 26], train_class_loss: 0.282772, train_domain_loss: 0.000892, test_domain_loss: 0.542778\n",
            "epoch: 25, [iter: 6 / all 26], train_class_loss: 0.219781, train_domain_loss: 0.019711, test_domain_loss: 0.529074\n",
            "epoch: 25, [iter: 7 / all 26], train_class_loss: 0.231765, train_domain_loss: 0.002881, test_domain_loss: 0.543984\n",
            "epoch: 25, [iter: 8 / all 26], train_class_loss: 0.210757, train_domain_loss: 0.031342, test_domain_loss: 0.541669\n",
            "epoch: 25, [iter: 9 / all 26], train_class_loss: 0.163626, train_domain_loss: 0.014537, test_domain_loss: 0.535780\n",
            "epoch: 25, [iter: 10 / all 26], train_class_loss: 0.277057, train_domain_loss: 0.006373, test_domain_loss: 0.537956\n",
            "epoch: 25, [iter: 11 / all 26], train_class_loss: 0.169392, train_domain_loss: 0.002116, test_domain_loss: 0.544368\n",
            "epoch: 25, [iter: 12 / all 26], train_class_loss: 0.234023, train_domain_loss: 0.011094, test_domain_loss: 0.547000\n",
            "epoch: 25, [iter: 13 / all 26], train_class_loss: 0.230443, train_domain_loss: 0.021429, test_domain_loss: 0.525266\n",
            "epoch: 25, [iter: 14 / all 26], train_class_loss: 0.250218, train_domain_loss: 0.020586, test_domain_loss: 0.540044\n",
            "epoch: 25, [iter: 15 / all 26], train_class_loss: 0.284218, train_domain_loss: 0.005843, test_domain_loss: 0.548218\n",
            "epoch: 25, [iter: 16 / all 26], train_class_loss: 0.167917, train_domain_loss: 0.048082, test_domain_loss: 0.541385\n",
            "epoch: 25, [iter: 17 / all 26], train_class_loss: 0.222094, train_domain_loss: 0.042330, test_domain_loss: 0.539255\n",
            "epoch: 25, [iter: 18 / all 26], train_class_loss: 0.266306, train_domain_loss: 0.020996, test_domain_loss: 0.542158\n",
            "epoch: 25, [iter: 19 / all 26], train_class_loss: 0.126316, train_domain_loss: 0.047128, test_domain_loss: 0.546421\n",
            "epoch: 25, [iter: 20 / all 26], train_class_loss: 0.213107, train_domain_loss: 0.012247, test_domain_loss: 0.540866\n",
            "epoch: 25, [iter: 21 / all 26], train_class_loss: 0.242078, train_domain_loss: 0.002043, test_domain_loss: 0.546415\n",
            "epoch: 25, [iter: 22 / all 26], train_class_loss: 0.077601, train_domain_loss: 0.000670, test_domain_loss: 0.549876\n",
            "epoch: 25, [iter: 23 / all 26], train_class_loss: 0.217159, train_domain_loss: 0.020917, test_domain_loss: 0.544586\n",
            "epoch: 25, [iter: 24 / all 26], train_class_loss: 0.177264, train_domain_loss: 0.009220, test_domain_loss: 0.531526\n",
            "epoch: 25, [iter: 25 / all 26], train_class_loss: 0.309320, train_domain_loss: 0.037310, test_domain_loss: 0.509032\n",
            "epoch: 25, [iter: 26 / all 26], train_class_loss: 0.153719, train_domain_loss: 0.002450, test_domain_loss: 0.530430\n",
            "Train class loss: 0.20524936599227098\n",
            "Train domain loss: 0.01641869908556915\n",
            "Val domain loss: 0.5378988660298861\n",
            "Total loss: 0.7595669311077262\n",
            "Training Accuracy: 0.9633413461538461\n",
            "Training Loss: 0.15159051392513972\n",
            "epoch: 25, accuracy on the training dataset: 0.963341\n",
            "epoch: 25, loss on the training dataset: 0.151591\n",
            "Validation Accuracy: 0.2517179944006108\n",
            "Validation Loss: 1.9283671811143386\n",
            "epoch: 25, accuracy on the validation dataset: 0.251718\n",
            "epoch: 25, loss on the validation dataset: 1.928367\n",
            "\n",
            "Starting epoch 27/30, LR = [1e-05]\n",
            "epoch: 26, [iter: 1 / all 26], train_class_loss: 0.193023, train_domain_loss: 0.012325, test_domain_loss: 0.514801\n",
            "epoch: 26, [iter: 2 / all 26], train_class_loss: 0.107400, train_domain_loss: 0.006284, test_domain_loss: 0.536062\n",
            "epoch: 26, [iter: 3 / all 26], train_class_loss: 0.179611, train_domain_loss: 0.019111, test_domain_loss: 0.540605\n",
            "epoch: 26, [iter: 4 / all 26], train_class_loss: 0.251462, train_domain_loss: 0.028230, test_domain_loss: 0.544683\n",
            "epoch: 26, [iter: 5 / all 26], train_class_loss: 0.258315, train_domain_loss: 0.002812, test_domain_loss: 0.542228\n",
            "epoch: 26, [iter: 6 / all 26], train_class_loss: 0.237387, train_domain_loss: 0.039212, test_domain_loss: 0.534498\n",
            "epoch: 26, [iter: 7 / all 26], train_class_loss: 0.225813, train_domain_loss: 0.029444, test_domain_loss: 0.546648\n",
            "epoch: 26, [iter: 8 / all 26], train_class_loss: 0.136311, train_domain_loss: 0.002269, test_domain_loss: 0.540721\n",
            "epoch: 26, [iter: 9 / all 26], train_class_loss: 0.210795, train_domain_loss: 0.019637, test_domain_loss: 0.536102\n",
            "epoch: 26, [iter: 10 / all 26], train_class_loss: 0.193323, train_domain_loss: 0.006494, test_domain_loss: 0.537635\n",
            "epoch: 26, [iter: 11 / all 26], train_class_loss: 0.199099, train_domain_loss: 0.009455, test_domain_loss: 0.535526\n",
            "epoch: 26, [iter: 12 / all 26], train_class_loss: 0.305379, train_domain_loss: 0.006840, test_domain_loss: 0.546500\n",
            "epoch: 26, [iter: 13 / all 26], train_class_loss: 0.208090, train_domain_loss: 0.035763, test_domain_loss: 0.523048\n",
            "epoch: 26, [iter: 14 / all 26], train_class_loss: 0.191415, train_domain_loss: 0.008257, test_domain_loss: 0.542177\n",
            "epoch: 26, [iter: 15 / all 26], train_class_loss: 0.144267, train_domain_loss: 0.056767, test_domain_loss: 0.553741\n",
            "epoch: 26, [iter: 16 / all 26], train_class_loss: 0.370069, train_domain_loss: 0.002339, test_domain_loss: 0.538177\n",
            "epoch: 26, [iter: 17 / all 26], train_class_loss: 0.125513, train_domain_loss: 0.006105, test_domain_loss: 0.546506\n",
            "epoch: 26, [iter: 18 / all 26], train_class_loss: 0.208816, train_domain_loss: 0.034198, test_domain_loss: 0.542711\n",
            "epoch: 26, [iter: 19 / all 26], train_class_loss: 0.165735, train_domain_loss: 0.012649, test_domain_loss: 0.549955\n",
            "epoch: 26, [iter: 20 / all 26], train_class_loss: 0.189516, train_domain_loss: 0.027607, test_domain_loss: 0.538560\n",
            "epoch: 26, [iter: 21 / all 26], train_class_loss: 0.177992, train_domain_loss: 0.007501, test_domain_loss: 0.546478\n",
            "epoch: 26, [iter: 22 / all 26], train_class_loss: 0.168174, train_domain_loss: 0.010232, test_domain_loss: 0.546719\n",
            "epoch: 26, [iter: 23 / all 26], train_class_loss: 0.231031, train_domain_loss: 0.007494, test_domain_loss: 0.544211\n",
            "epoch: 26, [iter: 24 / all 26], train_class_loss: 0.207912, train_domain_loss: 0.001039, test_domain_loss: 0.530579\n",
            "epoch: 26, [iter: 25 / all 26], train_class_loss: 0.163713, train_domain_loss: 0.001906, test_domain_loss: 0.515355\n",
            "epoch: 26, [iter: 26 / all 26], train_class_loss: 0.218630, train_domain_loss: 0.015852, test_domain_loss: 0.534612\n",
            "Train class loss: 0.2026458537349334\n",
            "Train domain loss: 0.015762342211718742\n",
            "Val domain loss: 0.538801372051239\n",
            "Total loss: 0.7572095679978912\n",
            "Training Accuracy: 0.9639423076923077\n",
            "Training Loss: 0.14972091638124907\n",
            "epoch: 26, accuracy on the training dataset: 0.963942\n",
            "epoch: 26, loss on the training dataset: 0.149721\n",
            "Validation Accuracy: 0.2537541359124459\n",
            "Validation Loss: 1.9284308947931874\n",
            "epoch: 26, accuracy on the validation dataset: 0.253754\n",
            "epoch: 26, loss on the validation dataset: 1.928431\n",
            "\n",
            "Starting epoch 28/30, LR = [1e-05]\n",
            "epoch: 27, [iter: 1 / all 26], train_class_loss: 0.163654, train_domain_loss: 0.007542, test_domain_loss: 0.514325\n",
            "epoch: 27, [iter: 2 / all 26], train_class_loss: 0.204130, train_domain_loss: 0.017045, test_domain_loss: 0.535706\n",
            "epoch: 27, [iter: 3 / all 26], train_class_loss: 0.261181, train_domain_loss: 0.000802, test_domain_loss: 0.539364\n",
            "epoch: 27, [iter: 4 / all 26], train_class_loss: 0.169961, train_domain_loss: 0.021581, test_domain_loss: 0.539113\n",
            "epoch: 27, [iter: 5 / all 26], train_class_loss: 0.212724, train_domain_loss: 0.001709, test_domain_loss: 0.539392\n",
            "epoch: 27, [iter: 6 / all 26], train_class_loss: 0.190720, train_domain_loss: 0.003328, test_domain_loss: 0.532368\n",
            "epoch: 27, [iter: 7 / all 26], train_class_loss: 0.151838, train_domain_loss: 0.022962, test_domain_loss: 0.544803\n",
            "epoch: 27, [iter: 8 / all 26], train_class_loss: 0.241192, train_domain_loss: 0.006486, test_domain_loss: 0.537448\n",
            "epoch: 27, [iter: 9 / all 26], train_class_loss: 0.245289, train_domain_loss: 0.034226, test_domain_loss: 0.541531\n",
            "epoch: 27, [iter: 10 / all 26], train_class_loss: 0.152971, train_domain_loss: 0.004494, test_domain_loss: 0.542677\n",
            "epoch: 27, [iter: 11 / all 26], train_class_loss: 0.166633, train_domain_loss: 0.003436, test_domain_loss: 0.539372\n",
            "epoch: 27, [iter: 12 / all 26], train_class_loss: 0.233070, train_domain_loss: 0.014654, test_domain_loss: 0.542566\n",
            "epoch: 27, [iter: 13 / all 26], train_class_loss: 0.129956, train_domain_loss: 0.002166, test_domain_loss: 0.525066\n",
            "epoch: 27, [iter: 14 / all 26], train_class_loss: 0.259372, train_domain_loss: 0.006435, test_domain_loss: 0.543346\n",
            "epoch: 27, [iter: 15 / all 26], train_class_loss: 0.187982, train_domain_loss: 0.031465, test_domain_loss: 0.553897\n",
            "epoch: 27, [iter: 16 / all 26], train_class_loss: 0.173672, train_domain_loss: 0.033810, test_domain_loss: 0.538677\n",
            "epoch: 27, [iter: 17 / all 26], train_class_loss: 0.236767, train_domain_loss: 0.015793, test_domain_loss: 0.546158\n",
            "epoch: 27, [iter: 18 / all 26], train_class_loss: 0.145590, train_domain_loss: 0.000651, test_domain_loss: 0.537865\n",
            "epoch: 27, [iter: 19 / all 26], train_class_loss: 0.232562, train_domain_loss: 0.031863, test_domain_loss: 0.547166\n",
            "epoch: 27, [iter: 20 / all 26], train_class_loss: 0.182442, train_domain_loss: 0.014904, test_domain_loss: 0.540808\n",
            "epoch: 27, [iter: 21 / all 26], train_class_loss: 0.189622, train_domain_loss: 0.016364, test_domain_loss: 0.547221\n",
            "epoch: 27, [iter: 22 / all 26], train_class_loss: 0.263928, train_domain_loss: 0.003204, test_domain_loss: 0.546025\n",
            "epoch: 27, [iter: 23 / all 26], train_class_loss: 0.158220, train_domain_loss: 0.007794, test_domain_loss: 0.542187\n",
            "epoch: 27, [iter: 24 / all 26], train_class_loss: 0.243375, train_domain_loss: 0.061704, test_domain_loss: 0.533620\n",
            "epoch: 27, [iter: 25 / all 26], train_class_loss: 0.225127, train_domain_loss: 0.001251, test_domain_loss: 0.514901\n",
            "epoch: 27, [iter: 26 / all 26], train_class_loss: 0.355531, train_domain_loss: 0.025255, test_domain_loss: 0.539253\n",
            "Train class loss: 0.20682725998071524\n",
            "Train domain loss: 0.015035504905077128\n",
            "Val domain loss: 0.5386482454263247\n",
            "Total loss: 0.7605110103121171\n",
            "Training Accuracy: 0.9633413461538461\n",
            "Training Loss: 0.15062166865055376\n",
            "epoch: 27, accuracy on the training dataset: 0.963341\n",
            "epoch: 27, loss on the training dataset: 0.150622\n",
            "Validation Accuracy: 0.2585899720030542\n",
            "Validation Loss: 1.9285919197827206\n",
            "epoch: 27, accuracy on the validation dataset: 0.258590\n",
            "epoch: 27, loss on the validation dataset: 1.928592\n",
            "\n",
            "Starting epoch 29/30, LR = [1e-05]\n",
            "epoch: 28, [iter: 1 / all 26], train_class_loss: 0.171928, train_domain_loss: 0.011059, test_domain_loss: 0.518146\n",
            "epoch: 28, [iter: 2 / all 26], train_class_loss: 0.184837, train_domain_loss: 0.004752, test_domain_loss: 0.535928\n",
            "epoch: 28, [iter: 3 / all 26], train_class_loss: 0.267064, train_domain_loss: 0.012852, test_domain_loss: 0.539850\n",
            "epoch: 28, [iter: 4 / all 26], train_class_loss: 0.149639, train_domain_loss: 0.045214, test_domain_loss: 0.543636\n",
            "epoch: 28, [iter: 5 / all 26], train_class_loss: 0.149002, train_domain_loss: 0.012863, test_domain_loss: 0.546730\n",
            "epoch: 28, [iter: 6 / all 26], train_class_loss: 0.137073, train_domain_loss: 0.007944, test_domain_loss: 0.533338\n",
            "epoch: 28, [iter: 7 / all 26], train_class_loss: 0.120757, train_domain_loss: 0.011106, test_domain_loss: 0.548422\n",
            "epoch: 28, [iter: 8 / all 26], train_class_loss: 0.194354, train_domain_loss: 0.000196, test_domain_loss: 0.544310\n",
            "epoch: 28, [iter: 9 / all 26], train_class_loss: 0.148100, train_domain_loss: 0.007488, test_domain_loss: 0.537952\n",
            "epoch: 28, [iter: 10 / all 26], train_class_loss: 0.182912, train_domain_loss: 0.001443, test_domain_loss: 0.536352\n",
            "epoch: 28, [iter: 11 / all 26], train_class_loss: 0.339305, train_domain_loss: 0.004674, test_domain_loss: 0.541433\n",
            "epoch: 28, [iter: 12 / all 26], train_class_loss: 0.196214, train_domain_loss: 0.001260, test_domain_loss: 0.548998\n",
            "epoch: 28, [iter: 13 / all 26], train_class_loss: 0.187676, train_domain_loss: 0.004415, test_domain_loss: 0.530507\n",
            "epoch: 28, [iter: 14 / all 26], train_class_loss: 0.159990, train_domain_loss: 0.030053, test_domain_loss: 0.543096\n",
            "epoch: 28, [iter: 15 / all 26], train_class_loss: 0.164174, train_domain_loss: 0.009291, test_domain_loss: 0.551845\n",
            "epoch: 28, [iter: 16 / all 26], train_class_loss: 0.231342, train_domain_loss: 0.031574, test_domain_loss: 0.541032\n",
            "epoch: 28, [iter: 17 / all 26], train_class_loss: 0.196131, train_domain_loss: 0.010811, test_domain_loss: 0.546687\n",
            "epoch: 28, [iter: 18 / all 26], train_class_loss: 0.263767, train_domain_loss: 0.032180, test_domain_loss: 0.541091\n",
            "epoch: 28, [iter: 19 / all 26], train_class_loss: 0.142924, train_domain_loss: 0.009145, test_domain_loss: 0.545552\n",
            "epoch: 28, [iter: 20 / all 26], train_class_loss: 0.189550, train_domain_loss: 0.041088, test_domain_loss: 0.545708\n",
            "epoch: 28, [iter: 21 / all 26], train_class_loss: 0.298044, train_domain_loss: 0.006145, test_domain_loss: 0.547500\n",
            "epoch: 28, [iter: 22 / all 26], train_class_loss: 0.187222, train_domain_loss: 0.000448, test_domain_loss: 0.548108\n",
            "epoch: 28, [iter: 23 / all 26], train_class_loss: 0.271441, train_domain_loss: 0.005371, test_domain_loss: 0.545575\n",
            "epoch: 28, [iter: 24 / all 26], train_class_loss: 0.154076, train_domain_loss: 0.011970, test_domain_loss: 0.533092\n",
            "epoch: 28, [iter: 25 / all 26], train_class_loss: 0.233400, train_domain_loss: 0.075704, test_domain_loss: 0.515392\n",
            "epoch: 28, [iter: 26 / all 26], train_class_loss: 0.180127, train_domain_loss: 0.001365, test_domain_loss: 0.536795\n",
            "Train class loss: 0.19619418737980035\n",
            "Train domain loss: 0.015015838219999121\n",
            "Val domain loss: 0.5402721143685855\n",
            "Total loss: 0.7514821399683849\n",
            "Training Accuracy: 0.9633413461538461\n",
            "Training Loss: 0.14871906818678746\n",
            "epoch: 28, accuracy on the training dataset: 0.963341\n",
            "epoch: 28, loss on the training dataset: 0.148719\n",
            "Validation Accuracy: 0.26113514889284806\n",
            "Validation Loss: 1.9286791672079155\n",
            "epoch: 28, accuracy on the validation dataset: 0.261135\n",
            "epoch: 28, loss on the validation dataset: 1.928679\n",
            "\n",
            "Starting epoch 30/30, LR = [1e-05]\n",
            "epoch: 29, [iter: 1 / all 26], train_class_loss: 0.157028, train_domain_loss: 0.002245, test_domain_loss: 0.520932\n",
            "epoch: 29, [iter: 2 / all 26], train_class_loss: 0.280816, train_domain_loss: 0.002938, test_domain_loss: 0.537967\n",
            "epoch: 29, [iter: 3 / all 26], train_class_loss: 0.165049, train_domain_loss: 0.010468, test_domain_loss: 0.541516\n",
            "epoch: 29, [iter: 4 / all 26], train_class_loss: 0.159695, train_domain_loss: 0.002333, test_domain_loss: 0.545313\n",
            "epoch: 29, [iter: 5 / all 26], train_class_loss: 0.143769, train_domain_loss: 0.019762, test_domain_loss: 0.542527\n",
            "epoch: 29, [iter: 6 / all 26], train_class_loss: 0.180191, train_domain_loss: 0.004655, test_domain_loss: 0.532177\n",
            "epoch: 29, [iter: 7 / all 26], train_class_loss: 0.178649, train_domain_loss: 0.002121, test_domain_loss: 0.546152\n",
            "epoch: 29, [iter: 8 / all 26], train_class_loss: 0.239944, train_domain_loss: 0.005146, test_domain_loss: 0.540635\n",
            "epoch: 29, [iter: 9 / all 26], train_class_loss: 0.088397, train_domain_loss: 0.009325, test_domain_loss: 0.541712\n",
            "epoch: 29, [iter: 10 / all 26], train_class_loss: 0.252181, train_domain_loss: 0.007104, test_domain_loss: 0.539076\n",
            "epoch: 29, [iter: 11 / all 26], train_class_loss: 0.141179, train_domain_loss: 0.014132, test_domain_loss: 0.544720\n",
            "epoch: 29, [iter: 12 / all 26], train_class_loss: 0.143976, train_domain_loss: 0.014016, test_domain_loss: 0.541982\n",
            "epoch: 29, [iter: 13 / all 26], train_class_loss: 0.235893, train_domain_loss: 0.011467, test_domain_loss: 0.528182\n",
            "epoch: 29, [iter: 14 / all 26], train_class_loss: 0.168486, train_domain_loss: 0.019845, test_domain_loss: 0.544089\n",
            "epoch: 29, [iter: 15 / all 26], train_class_loss: 0.148515, train_domain_loss: 0.022205, test_domain_loss: 0.547826\n",
            "epoch: 29, [iter: 16 / all 26], train_class_loss: 0.114022, train_domain_loss: 0.017929, test_domain_loss: 0.544092\n",
            "epoch: 29, [iter: 17 / all 26], train_class_loss: 0.273777, train_domain_loss: 0.009367, test_domain_loss: 0.547716\n",
            "epoch: 29, [iter: 18 / all 26], train_class_loss: 0.251409, train_domain_loss: 0.006321, test_domain_loss: 0.538793\n",
            "epoch: 29, [iter: 19 / all 26], train_class_loss: 0.132600, train_domain_loss: 0.002827, test_domain_loss: 0.546092\n",
            "epoch: 29, [iter: 20 / all 26], train_class_loss: 0.187531, train_domain_loss: 0.016045, test_domain_loss: 0.541790\n",
            "epoch: 29, [iter: 21 / all 26], train_class_loss: 0.097957, train_domain_loss: 0.003829, test_domain_loss: 0.549395\n",
            "epoch: 29, [iter: 22 / all 26], train_class_loss: 0.346736, train_domain_loss: 0.007002, test_domain_loss: 0.552535\n",
            "epoch: 29, [iter: 23 / all 26], train_class_loss: 0.356282, train_domain_loss: 0.023689, test_domain_loss: 0.543230\n",
            "epoch: 29, [iter: 24 / all 26], train_class_loss: 0.193139, train_domain_loss: 0.030186, test_domain_loss: 0.539378\n",
            "epoch: 29, [iter: 25 / all 26], train_class_loss: 0.226248, train_domain_loss: 0.019367, test_domain_loss: 0.518076\n",
            "epoch: 29, [iter: 26 / all 26], train_class_loss: 0.277809, train_domain_loss: 0.042559, test_domain_loss: 0.537254\n",
            "Train class loss: 0.19774143483776313\n",
            "Train domain loss: 0.012572481655157529\n",
            "Val domain loss: 0.5405059250501486\n",
            "Total loss: 0.7508198415430692\n",
            "Training Accuracy: 0.9639423076923077\n",
            "Training Loss: 0.1467285339648907\n",
            "epoch: 29, accuracy on the training dataset: 0.963942\n",
            "epoch: 29, loss on the training dataset: 0.146729\n",
            "Validation Accuracy: 0.262407737337745\n",
            "Validation Loss: 1.9286880842690919\n",
            "epoch: 29, accuracy on the validation dataset: 0.262408\n",
            "epoch: 29, loss on the validation dataset: 1.928688\n",
            "\n",
            "Training complete in 10m 35s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjNs-9Oiq467"
      },
      "source": [
        "**Visualization Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2tBJQg0wjb7",
        "outputId": "03c07786-69e6-4186-de94-1fc23bd42433",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if TRAIN_1:\n",
        "  plot_loss(train_losses)\n",
        "\n",
        "if TRAIN_2:\n",
        "  plot_loss(train_loss)\n",
        "  plot_accuracy(train_acc)\n",
        "\n",
        "if TRAIN_DANN:\n",
        "  plot_loss_DANN(train_class_losses, train_domain_losses, test_domain_losses)\n",
        "\n",
        "if VALIDATION:\n",
        "  plot_loss_val(train_loss, val_loss)\n",
        "  plot_accuracy_val(train_acc, val_acc)\n",
        "\n",
        "if VALIDATION_DANN:\n",
        "  plot_loss_DANN(train_class_losses, train_domain_losses, val_domain_losses)\n",
        "  plot_loss(tot_losses)\n",
        "  plot_accuracy_val(train_acc, val_acc)\n",
        "  plot_loss_val(train_loss, val_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsHFI-GAJd69"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO3HV5pqJg1o",
        "outputId": "3f9b7fe7-6a3c-481e-94f6-06177b6795d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "if TEST_WITHOUT_DANN:\n",
        "\n",
        "  net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "  net.train(False) # Set Network to evaluation mode\n",
        "\n",
        "  running_corrects = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for images, labels in tqdm(test_dataloader):\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "\n",
        "      # Forward Pass\n",
        "      outputs = net(images)\n",
        "\n",
        "      # Get predictions\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "      # Update Corrects\n",
        "      total += labels.size(0)\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "  # Calculate Accuracy\n",
        "  accuracy = running_corrects / float(len(test_dataset))\n",
        "  accuracy_2 = running_corrects / float(total)\n",
        "\n",
        "  print('Test Accuracy: {}'.format(accuracy))\n",
        "  print('Test Accuracy: {}'.format(accuracy_2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [00:05<00:00,  6.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.5\n",
            "Test Accuracy: 0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeifD2yVmA1a",
        "outputId": "74fd523b-22be-4ead-f958-8da7822485cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if TEST_WITH_DANN:\n",
        "\n",
        "  net = net.eval()\n",
        "  net = net.to(DEVICE)\n",
        "\n",
        "  len_dataloader = len(test_dataloader)\n",
        "  data_test_iter = iter(test_dataloader)\n",
        "\n",
        "  i = 0\n",
        "  n_total = 0\n",
        "  n_correct = 0\n",
        "\n",
        "  while i < len_dataloader:\n",
        "\n",
        "    # Test model using target data\n",
        "    data_test = data_test_iter.next()\n",
        "    test_img, test_label = data_test\n",
        "\n",
        "    batch_size = len(test_label)\n",
        "\n",
        "    input_img = torch.FloatTensor(batch_size, 3, 224, 224)\n",
        "    class_label = torch.LongTensor(batch_size)\n",
        "\n",
        "    test_img = test_img.to(DEVICE)\n",
        "    test_label = test_label.to(DEVICE)\n",
        "    input_img = input_img.to(DEVICE)\n",
        "    class_label = class_label.to(DEVICE)\n",
        "\n",
        "    input_img.resize_as_(test_img).copy_(test_img)\n",
        "    class_label.resize_as_(test_label).copy_(test_label)\n",
        "\n",
        "    class_output = net(input_img)\n",
        "    \n",
        "    pred = class_output.data.max(1, keepdim=True)[1]\n",
        "    n_correct += pred.eq(class_label.data.view_as(pred)).cpu().sum()\n",
        "    n_total += batch_size\n",
        "\n",
        "    i+=1\n",
        "\n",
        "  accuracy = n_correct.data.numpy() * 1.0 / n_total\n",
        "\n",
        "  print('accuracy on Test dataset: {}'.format(accuracy))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy on Test dataset: 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}